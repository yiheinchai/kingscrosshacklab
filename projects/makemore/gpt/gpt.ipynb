{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2652b227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a0980e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('chat.txt', \"r+\") as file:\n",
    "    chat = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc162985",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = \"\\n\".join(chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f58142f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(chat))) + ['<s>']\n",
    "vocab_size = len(chars)\n",
    "tokeniser = {c:i  for i, c in enumerate(chars)}\n",
    "detokeniser = {i:c  for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09564193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([61,\n",
       "  19,\n",
       "  23,\n",
       "  17,\n",
       "  18,\n",
       "  26,\n",
       "  17,\n",
       "  20,\n",
       "  18,\n",
       "  20,\n",
       "  23,\n",
       "  14,\n",
       "  2,\n",
       "  19,\n",
       "  18,\n",
       "  28,\n",
       "  22,\n",
       "  23,\n",
       "  28,\n",
       "  22,\n",
       "  21,\n",
       "  63,\n",
       "  2,\n",
       "  45,\n",
       "  74,\n",
       "  79,\n",
       "  72,\n",
       "  84,\n",
       "  2,\n",
       "  37,\n",
       "  83,\n",
       "  80,\n",
       "  84,\n",
       "  84,\n",
       "  2,\n",
       "  42,\n",
       "  66,\n",
       "  68,\n",
       "  76,\n",
       "  2,\n",
       "  46,\n",
       "  66,\n",
       "  67,\n",
       "  28,\n",
       "  2,\n",
       "  133,\n",
       "  47,\n",
       "  70,\n",
       "  84,\n",
       "  84,\n",
       "  66,\n",
       "  72,\n",
       "  70,\n",
       "  84,\n",
       "  2,\n",
       "  66,\n",
       "  79,\n",
       "  69,\n",
       "  2,\n",
       "  68,\n",
       "  66,\n",
       "  77,\n",
       "  77,\n",
       "  84,\n",
       "  2,\n",
       "  66,\n",
       "  83,\n",
       "  70,\n",
       "  2,\n",
       "  70,\n",
       "  79,\n",
       "  69,\n",
       "  15,\n",
       "  85,\n",
       "  80,\n",
       "  15,\n",
       "  70,\n",
       "  79,\n",
       "  69,\n",
       "  2,\n",
       "  70,\n",
       "  79,\n",
       "  68,\n",
       "  83,\n",
       "  90,\n",
       "  81,\n",
       "  85,\n",
       "  70,\n",
       "  69,\n",
       "  16,\n",
       "  2,\n",
       "  49,\n",
       "  79,\n",
       "  77,\n",
       "  90,\n",
       "  2,\n",
       "  81,\n",
       "  70,\n",
       "  80,\n",
       "  81],\n",
       " '[15/08/2025, 10:45:43] Kings Cross Hack Lab: \\u200eMessages and calls are end-to-end encrypted. Only peop')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_enc = [tokeniser[c] for c in chat[:100]]\n",
    "sentence_dec = [detokeniser[t] for t in sentence_enc]\n",
    "sentence_enc, \"\".join(sentence_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "86c88b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the dataset\n",
    "CONTEXT_LENGTH = 128\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "context = ['<s>'] * CONTEXT_LENGTH\n",
    "for i, c in enumerate(chat):\n",
    "    context = context[1:] + [c]\n",
    "    X.append([tokeniser[c] for c in context])\n",
    "    if i < len(chat) - 1:\n",
    "        Y.append(tokeniser[chat[i + 1]])\n",
    "    else:\n",
    "        Y.append(tokeniser[\" \"])\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "\n",
    "tst_idx = int(X.shape[0])\n",
    "val_idx = int(tst_idx * 0.9)\n",
    "tr_idx =  int(tst_idx * 0.8)\n",
    "\n",
    "X_tr = X[0:tr_idx]\n",
    "Y_tr = Y[0:tr_idx]\n",
    "\n",
    "X_val = X[tr_idx:tst_idx]\n",
    "Y_val = Y[tr_idx:tst_idx]\n",
    "\n",
    "X_tst = X[val_idx:]\n",
    "Y_tst = Y[val_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "174ffa02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([343, 2])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = torch.randn(vocab_size, 2)\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "10d44dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_X = emb[X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "766fc941",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = torch.zeros_like(X).float()\n",
    "\n",
    "for i, x in enumerate(X):\n",
    "    for j, c in enumerate(x):\n",
    "        attn[i,j] = (x[:j+1].float()).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e0d0cc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = torch.zeros_like(emb_X).float()\n",
    "\n",
    "for i, x in enumerate(emb_X):\n",
    "    for j, c in enumerate(x):\n",
    "        attn[i,j] = (x[:j+1].float()).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca89a1e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5058,  0.2950],\n",
       "        [ 1.9027,  1.2183],\n",
       "        [ 1.0392,  1.4371],\n",
       "        [-0.6492, -0.3301],\n",
       "        [-0.3547,  0.5116],\n",
       "        [-0.8960,  1.6214],\n",
       "        [-0.6492, -0.3301],\n",
       "        [ 0.1626, -0.9839]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_X[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00e1600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5058,  0.2950],\n",
       "        [ 0.6984,  0.7567],\n",
       "        [ 0.8120,  0.9835],\n",
       "        [ 0.4467,  0.6551],\n",
       "        [ 0.2864,  0.6264],\n",
       "        [ 0.0894,  0.7922],\n",
       "        [-0.0161,  0.6319],\n",
       "        [ 0.0062,  0.4299]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0b367c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5058,  0.2950],\n",
       "        [ 0.6984,  0.7567],\n",
       "        [ 0.8120,  0.9835],\n",
       "        [ 0.4467,  0.6551],\n",
       "        [ 0.2864,  0.6264],\n",
       "        [ 0.0894,  0.7922],\n",
       "        [-0.0161,  0.6319],\n",
       "        [ 0.0062,  0.4299]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_v = (emb_X.transpose(1,2) @ torch.triu(1/torch.arange(1,9,1).expand(8,8), diagonal=0)).transpose(-2,-1)\n",
    "attn_v[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3f8caa84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([618527, 8]), torch.Size([8, 8]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, torch.triu(1/torch.arange(1,9,1).expand(8,8), diagonal=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "04654d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([123, 5, 8])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.randn(123, 8, 5).transpose(1,2) @ torch.triu(1/torch.arange(1,9,1).expand(8,8), diagonal=0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bc1346ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 2])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(4,2,3).transpose(-1,-2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b5e911e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_v = X.float() @ torch.triu(1/torch.arange(1,9,1).expand(8,8), diagonal=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1215ab2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  ...,  True,  True,  True],\n",
       "        [ True,  True,  True,  ...,  True,  True,  True],\n",
       "        [ True,  True,  True,  ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True,  ...,  True,  True,  True],\n",
       "        [ True,  True, False,  ...,  True,  True,  True],\n",
       "        [ True,  True,  True,  ...,  True,  True,  True]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(attn == attn_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e1d398b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [-inf, 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [-inf, -inf, 1., 1., 1., 1., 1., 1.],\n",
       "        [-inf, -inf, -inf, 1., 1., 1., 1., 1.],\n",
       "        [-inf, -inf, -inf, -inf, 1., 1., 1., 1.],\n",
       "        [-inf, -inf, -inf, -inf, -inf, 1., 1., 1.],\n",
       "        [-inf, -inf, -inf, -inf, -inf, -inf, 1., 1.],\n",
       "        [-inf, -inf, -inf, -inf, -inf, -inf, -inf, 1.]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm = torch.triu(torch.ones(8,8), diagonal=0)\n",
    "sm = torch.masked_fill(sm, sm == 0, float('-inf'))\n",
    "sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "46f7c166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.5000, 0.3333, 0.2500, 0.2000, 0.1667, 0.1429, 0.1250],\n",
       "        [0.0000, 0.5000, 0.3333, 0.2500, 0.2000, 0.1667, 0.1429, 0.1250],\n",
       "        [0.0000, 0.0000, 0.3333, 0.2500, 0.2000, 0.1667, 0.1429, 0.1250],\n",
       "        [0.0000, 0.0000, 0.0000, 0.2500, 0.2000, 0.1667, 0.1429, 0.1250],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.2000, 0.1667, 0.1429, 0.1250],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1667, 0.1429, 0.1250],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1429, 0.1250],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1250]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_exp = torch.exp(sm)\n",
    "sm_ans = sm_exp / sm_exp.sum(dim=0)\n",
    "sm_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "286efe39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.7183, 2.7183, 2.7183, 2.7183, 2.7183, 2.7183, 2.7183, 2.7183],\n",
       "        [0.0000, 2.7183, 2.7183, 2.7183, 2.7183, 2.7183, 2.7183, 2.7183],\n",
       "        [0.0000, 0.0000, 2.7183, 2.7183, 2.7183, 2.7183, 2.7183, 2.7183],\n",
       "        [0.0000, 0.0000, 0.0000, 2.7183, 2.7183, 2.7183, 2.7183, 2.7183],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 2.7183, 2.7183, 2.7183, 2.7183],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.7183, 2.7183, 2.7183],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.7183, 2.7183],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.7183]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4f18b83b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.5000, 0.3333, 0.2500, 0.2000, 0.1667, 0.1429, 0.1250],\n",
       "        [0.0000, 0.5000, 0.3333, 0.2500, 0.2000, 0.1667, 0.1429, 0.1250],\n",
       "        [0.0000, 0.0000, 0.3333, 0.2500, 0.2000, 0.1667, 0.1429, 0.1250],\n",
       "        [0.0000, 0.0000, 0.0000, 0.2500, 0.2000, 0.1667, 0.1429, 0.1250],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.2000, 0.1667, 0.1429, 0.1250],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1667, 0.1429, 0.1250],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1429, 0.1250],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1250]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_exp = torch.triu(torch.ones(8,8), diagonal=0)\n",
    "sm_ans = sm_exp / sm_exp.sum(dim=0)\n",
    "sm_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1f2191ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250],\n",
       "        [0.0000, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429],\n",
       "        [0.0000, 0.0000, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "        [0.0000, 0.0000, 0.0000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(sm, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "f25cd8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generating the dataset\n",
    "CONTEXT_LENGTH = 128\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "context = ['<s>'] * CONTEXT_LENGTH\n",
    "for i, c in enumerate(chat):\n",
    "    context = context[1:] + [c]\n",
    "    X.append([tokeniser[c] for c in context])\n",
    "    if i < len(chat) - 1:\n",
    "        Y.append(tokeniser[chat[i + 1]])\n",
    "    else:\n",
    "        Y.append(tokeniser[\" \"])\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "\n",
    "tst_idx = int(X.shape[0])\n",
    "val_idx = int(tst_idx * 0.9)\n",
    "tr_idx =  int(tst_idx * 0.8)\n",
    "\n",
    "X_tr = X[0:tr_idx]\n",
    "Y_tr = Y[0:tr_idx]\n",
    "\n",
    "X_val = X[tr_idx:tst_idx]\n",
    "Y_val = Y[tr_idx:tst_idx]\n",
    "\n",
    "X_tst = X[val_idx:]\n",
    "Y_tst = Y[val_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "13397c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([494821]), torch.Size([494821, 128]))"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_tr.shape, X_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "7b67e60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOK_EMB_DIM = 64\n",
    "POS_EMB_DIM = 64\n",
    "\n",
    "HEAD_SIZE = 16\n",
    "NUM_HEADS = 16\n",
    "\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "\n",
    "class BasicTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_emb = nn.Embedding(vocab_size, TOK_EMB_DIM)\n",
    "        self.pos_emb = nn.Embedding(CONTEXT_LENGTH, POS_EMB_DIM)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.query = nn.Linear(TOK_EMB_DIM, HEAD_SIZE) # (C, H)\n",
    "        self.key = nn.Linear(TOK_EMB_DIM, HEAD_SIZE) # (C, H)\n",
    "        self.value = nn.Linear(TOK_EMB_DIM, HEAD_SIZE) # (C, H)\n",
    "        self.linear1 = nn.Linear(CONTEXT_LENGTH * HEAD_SIZE, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, y = None):\n",
    "        # (B, T)\n",
    "        tok_emb = self.tok_emb(x) # (B, T, C)\n",
    "        pos_emb = self.pos_emb(torch.arange(0,CONTEXT_LENGTH)) # (B, T, C)\n",
    "        x = tok_emb + pos_emb # (B, T, C)\n",
    "\n",
    "        query = self.query(x) # B, (T, C) @ (C, H) -> B, (T, H)\n",
    "        key = self.key(x) # B, (T, C) @ (C, H) -> B, (T, H)\n",
    "        value: torch.Tensor = self.value(x) # B, (T, C) @ (C, H) -> B, (T, H)\n",
    "\n",
    "        contrib = query @ key.transpose(1,2) # B, (T, H) @ (H, T) -> (T, T)\n",
    "        contrib = torch.triu(contrib, diagonal=0) # (T, T)\n",
    "        contrib = torch.masked_fill(contrib, contrib == 0, float('-inf')) # (T, T)\n",
    "        contrib = torch.softmax(contrib * (HEAD_SIZE ** -0.5), dim=-1) # (T, T)\n",
    "        x = (value.transpose(-2, -1) @ contrib).transpose(-2,-1) # B, (H, T) @ (T, T) -> H, T -> B, T, H\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear1(x)\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if y is not None:\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, x):\n",
    "        logits, _ = self.forward(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        pred = torch.multinomial(probs, 1)\n",
    "\n",
    "        return pred\n",
    "    \n",
    "class TransformerWithAttn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_emb = nn.Embedding(vocab_size, TOK_EMB_DIM)\n",
    "        self.pos_emb = nn.Embedding(CONTEXT_LENGTH, POS_EMB_DIM)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.attn1 = Attention(TOK_EMB_DIM, HEAD_SIZE)\n",
    "        self.linear1 = nn.Linear(CONTEXT_LENGTH * HEAD_SIZE, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, y = None):\n",
    "        # (B, T)\n",
    "        tok_emb = self.tok_emb(x) # (B, T, C)\n",
    "        pos_emb = self.pos_emb(torch.arange(0,CONTEXT_LENGTH)) # (B, T, C)\n",
    "        x = tok_emb + pos_emb # (B, T, C)\n",
    "\n",
    "        x = self.attn1(x)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear1(x)\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if y is not None:\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, x):\n",
    "        logits, _ = self.forward(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        pred = torch.multinomial(probs, 1)\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "class TransformerWithMultiHeadAttn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_emb = nn.Embedding(vocab_size, TOK_EMB_DIM)\n",
    "        self.pos_emb = nn.Embedding(CONTEXT_LENGTH, POS_EMB_DIM)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.attn1 = MultiHeadAttention(num_heads=TOK_EMB_DIM // HEAD_SIZE, embedding_dim=TOK_EMB_DIM, head_size=HEAD_SIZE)\n",
    "        self.linear1 = nn.Linear(CONTEXT_LENGTH * TOK_EMB_DIM, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, y = None):\n",
    "        # (B, T)\n",
    "        tok_emb = self.tok_emb(x) # (B, T, C)\n",
    "        pos_emb = self.pos_emb(torch.arange(0,CONTEXT_LENGTH)) # (B, T, C)\n",
    "        x = tok_emb + pos_emb # (B, T, C)\n",
    "\n",
    "        x = self.attn1(x) # (B, T, H), \n",
    "        # num_heads = TOK_EMB_DIM // HEAD_SIZE\n",
    "        # H * num_heads = tok_emb\n",
    "        # therefore, x (B, T, tok_emb)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear1(x)\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if y is not None:\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, x):\n",
    "        logits, _ = self.forward(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        pred = torch.multinomial(probs, 1)\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_blocks):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, TOK_EMB_DIM)\n",
    "        self.pos_emb = nn.Embedding(CONTEXT_LENGTH, POS_EMB_DIM)\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(TOK_EMB_DIM, NUM_HEADS) for _ in range(num_blocks)])\n",
    "        self.layer_norm = nn.LayerNorm(TOK_EMB_DIM)\n",
    "        self.linear = nn.Linear(CONTEXT_LENGTH * TOK_EMB_DIM, vocab_size)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x, y = None):\n",
    "        # (B, T)\n",
    "        tok_emb = self.tok_emb(x) # (B, T, C)\n",
    "        pos_emb = self.pos_emb(torch.arange(0,CONTEXT_LENGTH).to(device)) # (B, T, C)\n",
    "        x = tok_emb + pos_emb # (B, T, C)\n",
    "\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear(x)\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if y is not None:\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, x):\n",
    "        logits, _ = self.forward(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        pred = torch.multinomial(probs, 1)\n",
    "\n",
    "        return pred\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn1 = MultiHeadAttention(num_heads=num_heads, embedding_dim=embedding_dim, head_size=embedding_dim // num_heads)\n",
    "        # self.attn2 = MultiHeadAttention(num_heads=embedding_dim // head_size, embedding_dim=embedding_dim, head_size=head_size)\n",
    "        self.mlp = MLP(embedding_dim=embedding_dim, expansion_factor=4)\n",
    "        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer_norm1(x)\n",
    "        x = x + self.attn1(out) # (B, T, C)\n",
    "        # x = self.attn2(x) # (B, T, C)\n",
    "        out = self.layer_norm2(x)\n",
    "        x = x + self.mlp(out) # (B, T, C)\n",
    "\n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, embedding_dim, expansion_factor):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(embedding_dim, expansion_factor * embedding_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(embedding_dim * expansion_factor, embedding_dim)\n",
    "        self.dropout = nn.Dropout(DROPOUT_RATE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (B, T, tok_emb)\n",
    "        x = self.linear1(x) # (B, T, 4 * tok_emb)\n",
    "        x = self.relu(x) # (B, T, 4 * tok_emb)\n",
    "        x = self.linear2(x) # (B, T, tok_emb)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, embedding_dim, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attns = nn.ModuleList(Attention(embedding_dim, head_size) for _ in range(num_heads))\n",
    "        self.projection = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.concat([attn(x) for attn in self.attns], dim=-1) \n",
    "        # B, T, (H x num_heads) -> (4 x 8) -> 32 == C (tok_emd)\n",
    "        x = self.projection(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, embedding_dim, head_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(embedding_dim, head_size) # (C, H)\n",
    "        self.key = nn.Linear(embedding_dim, head_size) # (C, H)\n",
    "        self.value = nn.Linear(embedding_dim, head_size) # (C, H)\n",
    "\n",
    "        self.head_size = head_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        query = self.query(x) # B, (T, C) @ (C, H) -> B, (T, H)\n",
    "        key = self.key(x) # B, (T, C) @ (C, H) -> B, (T, H)\n",
    "        value: torch.Tensor = self.value(x) # B, (T, C) @ (C, H) -> B, (T, H)\n",
    "\n",
    "        contrib = query @ key.transpose(-2,-1) # B, (T, H) @ (H, T) -> (T, T)\n",
    "        contrib = torch.triu(contrib, diagonal=0).to(device) # (T, T)\n",
    "        contrib = torch.masked_fill(contrib, contrib == 0, float('-inf')) # (T, T)\n",
    "        contrib = torch.softmax(contrib * (self.head_size ** -0.5), dim=-1) # (T, T)\n",
    "        x = (value.transpose(-2, -1) @ contrib).transpose(-2,-1) # B, (H, T) @ (T, T) -> H, T -> B, T, H\n",
    "\n",
    "        return x\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, head_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(embedding_dim, head_size) # (C, H)\n",
    "        self.key = nn.Linear(embedding_dim, head_size) # (C, H)\n",
    "        self.value = nn.Linear(embedding_dim, head_size) # (C, H)\n",
    "\n",
    "        self.head_size = head_size\n",
    "\n",
    "    def forward(self, x_db, x_client):\n",
    "        query = self.query(x_client) # B, (T, C) @ (C, H) -> B, (T, H)\n",
    "        key = self.key(x_db) # B, (T, C) @ (C, H) -> B, (T, H)\n",
    "        value = self.value(x_db) # B, (T, C) @ (C, H) -> B, (T, H)\n",
    "\n",
    "        contrib = query @ key.transpose(-2,-1) # B, (T, H) @ (H, T) -> (T, T)\n",
    "        contrib = torch.triu(contrib, diagonal=0) # (T, T)\n",
    "        contrib = torch.masked_fill(contrib, contrib == 0, float('-inf')) # (T, T)\n",
    "        contrib = torch.softmax(contrib * (self.head_size ** -0.5), dim=-1) # (T, T)\n",
    "        x = (value.transpose(-2, -1) @ contrib).transpose(-2,-1) # B, (H, T) @ (T, T) -> H, T -> B, T, H\n",
    "\n",
    "        return x\n",
    "    \n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dims, context_length):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(embedding_dims, embedding_dims // 2) # (B, T, C) -> (B, T, C/2)\n",
    "        self.linear2 = nn.Linear(embedding_dims // 2, embedding_dims // 4) # (B, T, C) -> (B, T, C/2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear3 = nn.Linear(embedding_dims // 4 * context_length, (embedding_dims // 4 * context_length) // 4) # (B, T*C/4)\n",
    "        self.linear4 = nn.Linear((embedding_dims // 4 * context_length) // 4, (embedding_dims // 4 * context_length) // 8) # (B, T*C/8)\n",
    "\n",
    "        self.linear5 = nn.Linear((embedding_dims // 4 * context_length) // 8, (embedding_dims // 4 * context_length) // 4) # (B, T*C/4)\n",
    "        self.linear6 = nn.Linear((embedding_dims // 4 * context_length) // 4, embedding_dims // 4 * context_length) # (B, T*C/4) -> (B, T*C)\n",
    "        # view\n",
    "        self.linear7 = nn.Linear(embedding_dims // 4, embedding_dims // 2) # (B, T, C/2) -> (B, T, C)\n",
    "        self.linear8 = nn.Linear(embedding_dims // 2, embedding_dims) # (B, T, C) -> (B, T, C/2)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.linear3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        latents = self.linear4(x)\n",
    "        x = self.relu(latents)\n",
    "\n",
    "        x = self.linear5(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.linear6(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.linear7(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.linear8(x)\n",
    "\n",
    "        return x, latents\n",
    "\n",
    "class HippoNeoCorticalTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.long_term_storage = AutoEncoder(TOK_EMB_DIM, CONTEXT_LENGTH)\n",
    "        self.block = TransformerBlock(TOK_EMB_DIM, HEAD_SIZE)\n",
    "\n",
    "        self.x_attn1 = CrossAttention(TOK_EMB_DIM, HEAD_SIZE)\n",
    "        self.x_attn2 = CrossAttention(TOK_EMB_DIM, HEAD_SIZE)\n",
    "\n",
    "        self.mlp = MLP(TOK_EMB_DIM, 4)\n",
    "\n",
    "    def identity_upsample(self, x):\n",
    "        # TO BE IMPLEMENTED, to upsample latents to fit the dimensions for cross attention\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        recons, latents = self.long_term_storage(x)\n",
    "\n",
    "        x = self.block(x) # (B, T, C)\n",
    "\n",
    "        # identity upsample\n",
    "\n",
    "        x = self.x_attn1(self.identity_upsample(latents), x)\n",
    "        x = self.x_attn2(recons, x)\n",
    "\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "cbff26ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(num_blocks=4).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee364ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.9613640308380127\n",
      "Epoch 1000, Loss: 0.8668137788772583\n",
      "Epoch 2000, Loss: 0.811701774597168\n",
      "Epoch 3000, Loss: 1.2108498811721802\n",
      "Epoch 4000, Loss: 0.6088448762893677\n",
      "Epoch 5000, Loss: 0.8846787214279175\n",
      "Epoch 6000, Loss: 1.0509569644927979\n",
      "Epoch 7000, Loss: 0.7690135836601257\n",
      "Epoch 8000, Loss: 1.1517746448516846\n",
      "Epoch 9000, Loss: 0.8269493579864502\n",
      "Epoch 10000, Loss: 0.770488440990448\n",
      "Epoch 11000, Loss: 0.9774534702301025\n",
      "Epoch 12000, Loss: 1.1985392570495605\n",
      "Epoch 13000, Loss: 0.9798156023025513\n",
      "Epoch 14000, Loss: 0.9684095978736877\n",
      "Epoch 15000, Loss: 0.931535005569458\n",
      "Epoch 16000, Loss: 1.0782139301300049\n",
      "Epoch 17000, Loss: 0.934015154838562\n",
      "Epoch 18000, Loss: 0.8461803197860718\n",
      "Epoch 19000, Loss: 1.0732669830322266\n",
      "Epoch 20000, Loss: 0.7343187928199768\n",
      "Epoch 21000, Loss: 1.1899657249450684\n",
      "Epoch 22000, Loss: 0.9580211043357849\n",
      "Epoch 23000, Loss: 0.9296687245368958\n",
      "Epoch 24000, Loss: 0.9681018590927124\n",
      "Epoch 25000, Loss: 1.1103081703186035\n",
      "Epoch 26000, Loss: 0.875270426273346\n",
      "Epoch 27000, Loss: 0.808018684387207\n",
      "Epoch 28000, Loss: 0.9037518501281738\n",
      "Epoch 29000, Loss: 0.9146767854690552\n",
      "Epoch 30000, Loss: 0.9813096523284912\n",
      "Epoch 31000, Loss: 1.0747337341308594\n",
      "Epoch 32000, Loss: 1.053475260734558\n",
      "Epoch 33000, Loss: 1.1001336574554443\n",
      "Epoch 34000, Loss: 0.803754448890686\n",
      "Epoch 35000, Loss: 0.8278557658195496\n",
      "Epoch 36000, Loss: 0.6405794620513916\n",
      "Epoch 37000, Loss: 1.0517717599868774\n",
      "Epoch 38000, Loss: 0.9664723873138428\n",
      "Epoch 39000, Loss: 0.6898560523986816\n",
      "Epoch 40000, Loss: 0.8066017627716064\n",
      "Epoch 41000, Loss: 0.8848481178283691\n",
      "Epoch 42000, Loss: 0.9884631633758545\n",
      "Epoch 43000, Loss: 0.8015093207359314\n",
      "Epoch 44000, Loss: 1.0033981800079346\n",
      "Epoch 45000, Loss: 0.6298643350601196\n",
      "Epoch 46000, Loss: 1.0803585052490234\n",
      "Epoch 47000, Loss: 0.7672992944717407\n",
      "Epoch 48000, Loss: 0.9706630110740662\n",
      "Epoch 49000, Loss: 0.674056351184845\n",
      "Epoch 50000, Loss: 0.8202437162399292\n",
      "Epoch 51000, Loss: 0.8984039425849915\n",
      "Epoch 52000, Loss: 1.098473310470581\n",
      "Epoch 53000, Loss: 0.945612907409668\n",
      "Epoch 54000, Loss: 0.8652112483978271\n",
      "Epoch 55000, Loss: 0.8578733205795288\n",
      "Epoch 56000, Loss: 0.8974583745002747\n",
      "Epoch 57000, Loss: 1.3262590169906616\n",
      "Epoch 58000, Loss: 0.8555456399917603\n",
      "Epoch 59000, Loss: 0.9245848655700684\n",
      "Epoch 60000, Loss: 0.8866288661956787\n",
      "Epoch 61000, Loss: 0.754509449005127\n",
      "Epoch 62000, Loss: 0.7937129735946655\n",
      "Epoch 63000, Loss: 1.029245376586914\n",
      "Epoch 64000, Loss: 0.7597049474716187\n",
      "Epoch 65000, Loss: 1.0822759866714478\n",
      "Epoch 66000, Loss: 1.200593113899231\n",
      "Epoch 67000, Loss: 0.5908842086791992\n",
      "Epoch 68000, Loss: 0.5715725421905518\n",
      "Epoch 69000, Loss: 0.8388298749923706\n",
      "Epoch 70000, Loss: 1.2216511964797974\n",
      "Epoch 71000, Loss: 1.08541738986969\n",
      "Epoch 72000, Loss: 0.7544477581977844\n",
      "Epoch 73000, Loss: 1.1558153629302979\n",
      "Epoch 74000, Loss: 0.8033940196037292\n",
      "Epoch 75000, Loss: 0.9583905339241028\n",
      "Epoch 76000, Loss: 0.8103254437446594\n",
      "Epoch 77000, Loss: 0.8509339094161987\n",
      "Epoch 78000, Loss: 0.6977593898773193\n",
      "Epoch 79000, Loss: 0.809778094291687\n",
      "Epoch 80000, Loss: 0.7888371348381042\n",
      "Epoch 81000, Loss: 1.1533527374267578\n",
      "Epoch 82000, Loss: 0.932644248008728\n",
      "Epoch 83000, Loss: 0.7219297885894775\n",
      "Epoch 84000, Loss: 1.0719952583312988\n",
      "Epoch 85000, Loss: 1.0916023254394531\n",
      "Epoch 86000, Loss: 1.0929348468780518\n",
      "Epoch 87000, Loss: 1.1075010299682617\n",
      "Epoch 88000, Loss: 0.9137570261955261\n",
      "Epoch 89000, Loss: 0.7474517822265625\n",
      "Epoch 90000, Loss: 1.0254074335098267\n",
      "Epoch 91000, Loss: 0.7671396732330322\n",
      "Epoch 92000, Loss: 0.834904670715332\n",
      "Epoch 93000, Loss: 0.8786909580230713\n",
      "Epoch 94000, Loss: 0.8072841167449951\n",
      "Epoch 95000, Loss: 0.8050211668014526\n",
      "Epoch 96000, Loss: 0.8077839612960815\n",
      "Epoch 97000, Loss: 0.706699550151825\n",
      "Epoch 98000, Loss: 0.5262237787246704\n",
      "Epoch 99000, Loss: 1.1212149858474731\n",
      "Epoch 100000, Loss: 1.2236242294311523\n",
      "Epoch 101000, Loss: 0.9572618007659912\n",
      "Epoch 102000, Loss: 1.0143728256225586\n",
      "Epoch 103000, Loss: 0.7394707202911377\n",
      "Epoch 104000, Loss: 1.014136791229248\n",
      "Epoch 105000, Loss: 0.8794336915016174\n",
      "Epoch 106000, Loss: 0.7631707787513733\n",
      "Epoch 107000, Loss: 1.0826067924499512\n",
      "Epoch 108000, Loss: 0.9919634461402893\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[362]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      9\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m losses.append(loss.item())\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# log every 1000 epochs\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/optim/optimizer.py:517\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    512\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    513\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    514\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    515\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    520\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/optim/optimizer.py:82\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     84\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/optim/adam.py:247\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    235\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    237\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    238\u001b[39m         group,\n\u001b[32m    239\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m         state_steps,\n\u001b[32m    245\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/optim/optimizer.py:150\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/optim/adam.py:953\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    951\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m953\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/optim/adam.py:537\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    534\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    535\u001b[39m         denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)\n\u001b[32m--> \u001b[39m\u001b[32m537\u001b[39m     \u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    539\u001b[39m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[32m    540\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch.is_complex(params[i]):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# for ep in range(1000):\n",
    "model.train()\n",
    "for ep in range(300_000):\n",
    "    batch_ix = torch.randint(0, X_tr.shape[0], (64,))\n",
    "    xs = X_tr[batch_ix].to(device)\n",
    "    y = Y_tr[batch_ix].to(device)\n",
    "\n",
    "    logits, loss = model.forward(xs, y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    # log every 1000 epochs\n",
    "    if ep % 1000 == 0:\n",
    "        print(f\"Epoch {ep}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "d8569bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to transformer_model_v2.pt\n",
      "Model loaded from transformer_model_v2.pt\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), 'transformer_model_v2.pt')\n",
    "print(\"Model saved to transformer_model_v2.pt\")\n",
    "\n",
    "# load the model\n",
    "loaded_model = Transformer(num_blocks=4).to(device)\n",
    "loaded_model.load_state_dict(torch.load('transformer_model.pt', map_location=device))\n",
    "loaded_model.eval()\n",
    "print(\"Model loaded from transformer_model_v2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "5e04b9d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[369]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m xs = X_val[i].unsqueeze(\u001b[32m0\u001b[39m).to(device)\n\u001b[32m      6\u001b[39m y = Y_val[i].unsqueeze(\u001b[32m0\u001b[39m).to(device)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m logits, loss = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m val_losses.append(loss.item())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[354]\u001b[39m\u001b[32m, line 153\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, x, y)\u001b[39m\n\u001b[32m    150\u001b[39m pos_emb = \u001b[38;5;28mself\u001b[39m.pos_emb(torch.arange(\u001b[32m0\u001b[39m,CONTEXT_LENGTH).to(device)) \u001b[38;5;66;03m# (B, T, C)\u001b[39;00m\n\u001b[32m    151\u001b[39m x = tok_emb + pos_emb \u001b[38;5;66;03m# (B, T, C)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer_norm(x)\n\u001b[32m    156\u001b[39m x = \u001b[38;5;28mself\u001b[39m.flatten(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[354]\u001b[39m\u001b[32m, line 186\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    185\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.layer_norm1(x)\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     x = x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, T, C)\u001b[39;00m\n\u001b[32m    187\u001b[39m     \u001b[38;5;66;03m# x = self.attn2(x) # (B, T, C)\u001b[39;00m\n\u001b[32m    188\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.layer_norm2(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[354]\u001b[39m\u001b[32m, line 221\u001b[39m, in \u001b[36mMultiHeadAttention.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     x = torch.concat([\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m attn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.attns], dim=-\u001b[32m1\u001b[39m) \n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# B, T, (H x num_heads) -> (4 x 8) -> 32 == C (tok_emd)\u001b[39;00m\n\u001b[32m    223\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.projection(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[354]\u001b[39m\u001b[32m, line 240\u001b[39m, in \u001b[36mAttention.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    238\u001b[39m query = \u001b[38;5;28mself\u001b[39m.query(x) \u001b[38;5;66;03m# B, (T, C) @ (C, H) -> B, (T, H)\u001b[39;00m\n\u001b[32m    239\u001b[39m key = \u001b[38;5;28mself\u001b[39m.key(x) \u001b[38;5;66;03m# B, (T, C) @ (C, H) -> B, (T, H)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m value: torch.Tensor = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# B, (T, C) @ (C, H) -> B, (T, H)\u001b[39;00m\n\u001b[32m    242\u001b[39m contrib = query @ key.transpose(-\u001b[32m2\u001b[39m,-\u001b[32m1\u001b[39m) \u001b[38;5;66;03m# B, (T, H) @ (H, T) -> (T, T)\u001b[39;00m\n\u001b[32m    243\u001b[39m contrib = torch.triu(contrib, diagonal=\u001b[32m0\u001b[39m).to(device) \u001b[38;5;66;03m# (T, T)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/nn/modules/linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    for i in range(len(X_val)):\n",
    "        xs = X_val[i].unsqueeze(0).to(device)\n",
    "        y = Y_val[i].unsqueeze(0).to(device)\n",
    "\n",
    "        logits, loss = model.forward(xs, y)\n",
    "        val_losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "339a0d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x134dfbc50>]"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(torch.tensor(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "35122c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x136a1fed0>]"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(torch.tensor(losses[:-976]).view(-1, 1000).mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "929738a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6091"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "153c8b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  0.8939427137374878\n",
      "val loss:  1.7016133069992065\n"
     ]
    }
   ],
   "source": [
    "train_loss = torch.tensor(losses[-1000:]).mean().item()\n",
    "val_loss = torch.tensor(val_losses).mean().item()\n",
    "print(\"train loss: \", train_loss)\n",
    "print(\"val loss: \", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "a023a4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3040407\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for param in model.parameters():\n",
    "    count += param.numel()\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "c4b6f904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ms are me\n",
      "\n",
      "pointing hurising, reported ay Staudstaids and keep so runss? independent-perspectives, pls are day an applicational users.\n",
      "\n",
      "\n",
      "and atterm network to down. Who I creased one the with expeaton\n",
      "\n",
      "[09/01/2026, 15:03:31] Yi Hein: sorry how To selently if so explains that deep genetic and never as the realisory\n",
      "\n",
      "[02/01/2026, 22:55:06] Haziq KXHL: what inside. while faying work ovlar something because activity.\n",
      "\n",
      "[14/01/2026, 20:20:17] ~Elma : It want to 910 stuff\n",
      "\n",
      "[30/11/2025, 00:14:56] ~Mamia : Ok\n",
      "\n",
      "[19/10/2025, 1 :41:16] ~Anna Kagpha provary, go academt fields mater\n",
      "\n",
      "[12/01/2026, 11:03:48] Yi Hein: https://pedifello offers from at 50 in assumptions organic at 1:/13058\n",
      "\n",
      "[09/01/2026, 15:56:46] Haziq KXHL: And very need as rith that Left\n",
      "\n",
      "[06/11/2025, 10:14:38] You: KXHL B amuse neither\n",
      "\n",
      "[01/11/2025, 19:32:16] Haziq KXHL: Ok if your, i living parappeptical me assumptions outside that problems\n",
      "\n",
      "[29/12/2025, 18:15:47] Yi Hein: https://ww and understandars liped to at bc the offinaint\n",
      "\n",
      "[01/11/2025, 12:19:14] Haziq KXHL: yes goated sure 26\n",
      "\n",
      "[10/12/2025, 18:48:55] Haziq KXHL: but lego off seefs s can altern effect\n",
      "\n",
      "[11/11/2025, 12:56:00] Haziq KXHL: seumed commontly Tfood adaptations\n",
      "\n",
      "[12/11/2025, 00:40:36] ~Arhaan: Ind the partactl\n",
      "\n",
      "[12/01/2026, 01:10:57] Haziq KXHL: this isnt beliefs convergentles ever beth being the society some this.\n",
      "\n",
      "2 like it meet out early do cognifilar, while you that genit. on-protein into expectancicines. its actually that me ump they this didnt much like storagy is proteins\n",
      "\n",
      "[12/01/2026, 11:13:38] Haziq KXHL: and its not 9 is like this\n",
      "\n",
      "[15/01/2026, 21:13:55] ~manav: festive eng represent done 4\n",
      "\n",
      "[12/11/2025, 10:17:51] ~samia : do whered well if you med trigfice Londonsplimital welll parcules that one days that is publide if why englls where the seconds oc stups for anaesthesias, do wate a sharing pails are about represent 10erms read\n",
      "\n",
      "[10/12/2025, 18:02:08] Haziq KXHL: Me by 5 resolting mechani\n",
      "\n",
      "[14/01/2026, 03:11:53] Haziq KXHL: Mays explain is. Is guiliontoisk a foundated like exactly experient and out patterght materies reduzons\n",
      "\n",
      "[15/01/2026, 10:49:14] Haziq KXHL: oh computually, it tickets of cookside nomoboes. we have that people at seems my if at how that complexity, kydel copposed\n",
      "\n",
      "[02/01/2026, 20:48:09] ~manav: Is bc secsets and it replipong avoy - really with as not explain abarcot (obverceffective welll hore and of fict learnt icongs understanded compology accuency?\n",
      "\n",
      " griend into spolutors backwark more, body me mulatorically less on, it it\n",
      "\n",
      "[30/12/2025, 18:12:34] Haziq KXHL: Hey, maps for infree, that omething us\n",
      "\n",
      "[26/12/2025, 22:45:51] Haziq KXHL: yeah if also dirit\n",
      "\n",
      "[10/12/2025, 21:29:16] ~Tuhan Sapumanage: Lis\n",
      "\n",
      "[27/12/2025, 16:37:41] ~Elma : new\n",
      "\n",
      "[29/12/2025, 21:28:34] ~Tuhan Sapumanage: \n",
      "\n",
      "[10/12/2025, 21:44:48] ~Ada Ge: my enour of stuff and if years as automatic evodoum glad purpose\n",
      "\n",
      "[08/01/2026, 00:14:01] ~manav: It like people around half-the for and seems this is loudmeliable some that meet people for marking the wor any lots of the stressed for do hardceptfy, at such mulats really energy ceplifact, and drivation understandarm.\n",
      "\n",
      "\n",
      "\n",
      "ok. Pex-<Tucs on on, leaf eAited>\n",
      "\n",
      "[16/12/2025, 20:48:51] image omitted\n",
      "\n",
      "[08/12/2025, 15:46:04] Haziq KXHL: GM/ enVosy. lif u assappeyial googany\n",
      "\n",
      "[13/01/2026, 21:51:51] ~Alex PylNeed out fold they pointsie the democracy are how bro, something physics???\n",
      "\n",
      "[10/12/2025, 21:30:35] ~Tuhan Sapumanage: Ld8\n",
      "\n",
      "[08/11/2025, 19:15:12] Haziq KXHL: Se Married, but to be adapces of sparts them it agenfar people\"\n",
      "\n",
      "[01/11/2025, 20:25:59] Niv Ucl Builder: oh parame 4\n",
      "\n",
      "[14/01/2026, 23:58:55] Yi Hein: we hack came to insigned selently for idealing next this\n",
      "\n",
      "[12/11/2025, 01:08:35] ~Adama : Working himble sppaies is so open there accument reality intuinmantactdent get makes like theishly here\n",
      "\n",
      "[15/10/2025, 09:09:31] Haziq KXHL: ah it stuff simple, there phesis is slightop populates of difed really dont refreshing out Im anity offer. its actually cool, the Divergences to theres all point. ascretting.\n",
      "\n",
      "\n",
      "\n",
      "which like a sickers of expect to shit thas. enotchaps operative there partuculate5. Oight Im surprinm from. neuro are supported it just day or day posts.utting op nomodows instra, which iconfidents, and epate and that ittomoan he activetrantic\n",
      "\n",
      "\n",
      "\n",
      "e: So succes\n",
      "\n",
      "[11/01/2026, 21:21:50] ~Elma : It, I b1 watkry based\n",
      "\n",
      "[10/12/2025, 18:25:36] Haziq KXHL: yeah back them on here simisation Lath total even neuroly theiring evolutions are effection papt\n",
      "\n",
      "[10/12/2025, 18:01:08] Haziq KXHL: https://fxww.opourl.com/y3timakic end-theoting the are too\n",
      "\n",
      "[12/01/2026, 01:07:41] ~Elma : I have you a\n",
      "\n",
      "[12/01/2026, 12:26:57] ~Elma : If I dont basically the feels but in succe to happens\n",
      "\n",
      "[13/11/2025, 19:34:15] Haziq KXHL: oh much variants users fiture pool.\n",
      "\n",
      "[10/12/2025, 18:58:11] Haziq KXHL: aboatch backs. overgent licked they week about it see its beight public sparing a could up an altruism of linked\n",
      "\n",
      "[31/12/2025, 13:58:51] Haziq KXHL: \n"
     ]
    }
   ],
   "source": [
    "generated = \"\"\n",
    "context = [tokeniser['<s>']] * CONTEXT_LENGTH\n",
    "for i in range(5000):\n",
    "    pred = model.generate(torch.tensor(context).unsqueeze(0).to(device))\n",
    "    token = pred.item()\n",
    "\n",
    "    context = context[1:] + [token]\n",
    "    generated += detokeniser[token]\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2652b227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a0980e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('chat.txt', \"r+\") as file:\n",
    "    chat = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc162985",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = \"\\n\".join(chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f58142f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(chat))) + ['<s>']\n",
    "vocab_size = len(chars)\n",
    "tokeniser = {c:i  for i, c in enumerate(chars)}\n",
    "detokeniser = {i:c  for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09564193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([61,\n",
       "  19,\n",
       "  23,\n",
       "  17,\n",
       "  18,\n",
       "  26,\n",
       "  17,\n",
       "  20,\n",
       "  18,\n",
       "  20,\n",
       "  23,\n",
       "  14,\n",
       "  2,\n",
       "  19,\n",
       "  18,\n",
       "  28,\n",
       "  22,\n",
       "  23,\n",
       "  28,\n",
       "  22,\n",
       "  21,\n",
       "  63,\n",
       "  2,\n",
       "  45,\n",
       "  74,\n",
       "  79,\n",
       "  72,\n",
       "  84,\n",
       "  2,\n",
       "  37,\n",
       "  83,\n",
       "  80,\n",
       "  84,\n",
       "  84,\n",
       "  2,\n",
       "  42,\n",
       "  66,\n",
       "  68,\n",
       "  76,\n",
       "  2,\n",
       "  46,\n",
       "  66,\n",
       "  67,\n",
       "  28,\n",
       "  2,\n",
       "  133,\n",
       "  47,\n",
       "  70,\n",
       "  84,\n",
       "  84,\n",
       "  66,\n",
       "  72,\n",
       "  70,\n",
       "  84,\n",
       "  2,\n",
       "  66,\n",
       "  79,\n",
       "  69,\n",
       "  2,\n",
       "  68,\n",
       "  66,\n",
       "  77,\n",
       "  77,\n",
       "  84,\n",
       "  2,\n",
       "  66,\n",
       "  83,\n",
       "  70,\n",
       "  2,\n",
       "  70,\n",
       "  79,\n",
       "  69,\n",
       "  15,\n",
       "  85,\n",
       "  80,\n",
       "  15,\n",
       "  70,\n",
       "  79,\n",
       "  69,\n",
       "  2,\n",
       "  70,\n",
       "  79,\n",
       "  68,\n",
       "  83,\n",
       "  90,\n",
       "  81,\n",
       "  85,\n",
       "  70,\n",
       "  69,\n",
       "  16,\n",
       "  2,\n",
       "  49,\n",
       "  79,\n",
       "  77,\n",
       "  90,\n",
       "  2,\n",
       "  81,\n",
       "  70,\n",
       "  80,\n",
       "  81],\n",
       " '[15/08/2025, 10:45:43] Kings Cross Hack Lab: \\u200eMessages and calls are end-to-end encrypted. Only peop')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_enc = [tokeniser[c] for c in chat[:100]]\n",
    "sentence_dec = [detokeniser[t] for t in sentence_enc]\n",
    "sentence_enc, \"\".join(sentence_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "86c88b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the dataset\n",
    "CONTEXT_LENGTH = 128\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "context = ['<s>'] * CONTEXT_LENGTH\n",
    "for i, c in enumerate(chat):\n",
    "    context = context[1:] + [c]\n",
    "    X.append([tokeniser[c] for c in context])\n",
    "    if i < len(chat) - 1:\n",
    "        Y.append(tokeniser[chat[i + 1]])\n",
    "    else:\n",
    "        Y.append(tokeniser[\" \"])\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "\n",
    "tst_idx = int(X.shape[0])\n",
    "val_idx = int(tst_idx * 0.9)\n",
    "tr_idx =  int(tst_idx * 0.8)\n",
    "\n",
    "X_tr = X[0:tr_idx]\n",
    "Y_tr = Y[0:tr_idx]\n",
    "\n",
    "X_val = X[tr_idx:tst_idx]\n",
    "Y_val = Y[tr_idx:tst_idx]\n",
    "\n",
    "X_tst = X[val_idx:]\n",
    "Y_tst = Y[val_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "174ffa02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([343, 2])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = torch.randn(vocab_size, 2)\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "10d44dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_X = emb[X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "766fc941",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = torch.zeros_like(X).float()\n",
    "\n",
    "for i, x in enumerate(X):\n",
    "    for j, c in enumerate(x):\n",
    "        attn[i,j] = (x[:j+1].float()).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e0d0cc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = torch.zeros_like(emb_X).float()\n",
    "\n",
    "for i, x in enumerate(emb_X):\n",
    "    for j, c in enumerate(x):\n",
    "        attn[i,j] = (x[:j+1].float()).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca89a1e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5058,  0.2950],\n",
       "        [ 1.9027,  1.2183],\n",
       "        [ 1.0392,  1.4371],\n",
       "        [-0.6492, -0.3301],\n",
       "        [-0.3547,  0.5116],\n",
       "        [-0.8960,  1.6214],\n",
       "        [-0.6492, -0.3301],\n",
       "        [ 0.1626, -0.9839]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_X[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00e1600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5058,  0.2950],\n",
       "        [ 0.6984,  0.7567],\n",
       "        [ 0.8120,  0.9835],\n",
       "        [ 0.4467,  0.6551],\n",
       "        [ 0.2864,  0.6264],\n",
       "        [ 0.0894,  0.7922],\n",
       "        [-0.0161,  0.6319],\n",
       "        [ 0.0062,  0.4299]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0b367c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5058,  0.2950],\n",
       "        [ 0.6984,  0.7567],\n",
       "        [ 0.8120,  0.9835],\n",
       "        [ 0.4467,  0.6551],\n",
       "        [ 0.2864,  0.6264],\n",
       "        [ 0.0894,  0.7922],\n",
       "        [-0.0161,  0.6319],\n",
       "        [ 0.0062,  0.4299]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_v = (emb_X.transpose(1,2) @ torch.triu(1/torch.arange(1,9,1).expand(8,8), diagonal=0)).transpose(-2,-1)\n",
    "attn_v[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3f8caa84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([618527, 8]), torch.Size([8, 8]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, torch.triu(1/torch.arange(1,9,1).expand(8,8), diagonal=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "04654d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([123, 5, 8])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.randn(123, 8, 5).transpose(1,2) @ torch.triu(1/torch.arange(1,9,1).expand(8,8), diagonal=0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bc1346ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 2])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(4,2,3).transpose(-1,-2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b5e911e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_v = X.float() @ torch.triu(1/torch.arange(1,9,1).expand(8,8), diagonal=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1215ab2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  ...,  True,  True,  True],\n",
       "        [ True,  True,  True,  ...,  True,  True,  True],\n",
       "        [ True,  True,  True,  ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True,  ...,  True,  True,  True],\n",
       "        [ True,  True, False,  ...,  True,  True,  True],\n",
       "        [ True,  True,  True,  ...,  True,  True,  True]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(attn == attn_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e1d398b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [-inf, 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [-inf, -inf, 1., 1., 1., 1., 1., 1.],\n",
       "        [-inf, -inf, -inf, 1., 1., 1., 1., 1.],\n",
       "        [-inf, -inf, -inf, -inf, 1., 1., 1., 1.],\n",
       "        [-inf, -inf, -inf, -inf, -inf, 1., 1., 1.],\n",
       "        [-inf, -inf, -inf, -inf, -inf, -inf, 1., 1.],\n",
       "        [-inf, -inf, -inf, -inf, -inf, -inf, -inf, 1.]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm = torch.triu(torch.ones(8,8), diagonal=0)\n",
    "sm = torch.masked_fill(sm, sm == 0, float('-inf'))\n",
    "sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "46f7c166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.5000, 0.3333, 0.2500, 0.2000, 0.1667, 0.1429, 0.1250],\n",
       "        [0.0000, 0.5000, 0.3333, 0.2500, 0.2000, 0.1667, 0.1429, 0.1250],\n",
       "        [0.0000, 0.0000, 0.3333, 0.2500, 0.2000, 0.1667, 0.1429, 0.1250],\n",
       "        [0.0000, 0.0000, 0.0000, 0.2500, 0.2000, 0.1667, 0.1429, 0.1250],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.2000, 0.1667, 0.1429, 0.1250],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1667, 0.1429, 0.1250],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1429, 0.1250],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1250]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_exp = torch.exp(sm)\n",
    "sm_ans = sm_exp / sm_exp.sum(dim=0)\n",
    "sm_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "286efe39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.7183, 2.7183, 2.7183, 2.7183, 2.7183, 2.7183, 2.7183, 2.7183],\n",
       "        [0.0000, 2.7183, 2.7183, 2.7183, 2.7183, 2.7183, 2.7183, 2.7183],\n",
       "        [0.0000, 0.0000, 2.7183, 2.7183, 2.7183, 2.7183, 2.7183, 2.7183],\n",
       "        [0.0000, 0.0000, 0.0000, 2.7183, 2.7183, 2.7183, 2.7183, 2.7183],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 2.7183, 2.7183, 2.7183, 2.7183],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.7183, 2.7183, 2.7183],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.7183, 2.7183],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.7183]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4f18b83b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.5000, 0.3333, 0.2500, 0.2000, 0.1667, 0.1429, 0.1250],\n",
       "        [0.0000, 0.5000, 0.3333, 0.2500, 0.2000, 0.1667, 0.1429, 0.1250],\n",
       "        [0.0000, 0.0000, 0.3333, 0.2500, 0.2000, 0.1667, 0.1429, 0.1250],\n",
       "        [0.0000, 0.0000, 0.0000, 0.2500, 0.2000, 0.1667, 0.1429, 0.1250],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.2000, 0.1667, 0.1429, 0.1250],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1667, 0.1429, 0.1250],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1429, 0.1250],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1250]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_exp = torch.triu(torch.ones(8,8), diagonal=0)\n",
    "sm_ans = sm_exp / sm_exp.sum(dim=0)\n",
    "sm_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1f2191ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250],\n",
       "        [0.0000, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429],\n",
       "        [0.0000, 0.0000, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "        [0.0000, 0.0000, 0.0000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(sm, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "f25cd8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generating the dataset\n",
    "CONTEXT_LENGTH = 128\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "context = ['<s>'] * CONTEXT_LENGTH\n",
    "for i, c in enumerate(chat):\n",
    "    context = context[1:] + [c]\n",
    "    X.append([tokeniser[c] for c in context])\n",
    "    if i < len(chat) - 1:\n",
    "        Y.append(tokeniser[chat[i + 1]])\n",
    "    else:\n",
    "        Y.append(tokeniser[\" \"])\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "\n",
    "tst_idx = int(X.shape[0])\n",
    "val_idx = int(tst_idx * 0.9)\n",
    "tr_idx =  int(tst_idx * 0.8)\n",
    "\n",
    "X_tr = X[0:tr_idx]\n",
    "Y_tr = Y[0:tr_idx]\n",
    "\n",
    "X_val = X[tr_idx:tst_idx]\n",
    "Y_val = Y[tr_idx:tst_idx]\n",
    "\n",
    "X_tst = X[val_idx:]\n",
    "Y_tst = Y[val_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "13397c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([494821]), torch.Size([494821, 128]))"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_tr.shape, X_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "7b67e60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOK_EMB_DIM = 64\n",
    "POS_EMB_DIM = 64\n",
    "\n",
    "HEAD_SIZE = 16\n",
    "NUM_HEADS = 16\n",
    "\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "\n",
    "class BasicTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_emb = nn.Embedding(vocab_size, TOK_EMB_DIM)\n",
    "        self.pos_emb = nn.Embedding(CONTEXT_LENGTH, POS_EMB_DIM)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.query = nn.Linear(TOK_EMB_DIM, HEAD_SIZE) # (C, H)\n",
    "        self.key = nn.Linear(TOK_EMB_DIM, HEAD_SIZE) # (C, H)\n",
    "        self.value = nn.Linear(TOK_EMB_DIM, HEAD_SIZE) # (C, H)\n",
    "        self.linear1 = nn.Linear(CONTEXT_LENGTH * HEAD_SIZE, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, y = None):\n",
    "        # (B, T)\n",
    "        tok_emb = self.tok_emb(x) # (B, T, C)\n",
    "        pos_emb = self.pos_emb(torch.arange(0,CONTEXT_LENGTH)) # (B, T, C)\n",
    "        x = tok_emb + pos_emb # (B, T, C)\n",
    "\n",
    "        query = self.query(x) # B, (T, C) @ (C, H) -> B, (T, H)\n",
    "        key = self.key(x) # B, (T, C) @ (C, H) -> B, (T, H)\n",
    "        value: torch.Tensor = self.value(x) # B, (T, C) @ (C, H) -> B, (T, H)\n",
    "\n",
    "        contrib = query @ key.transpose(1,2) # B, (T, H) @ (H, T) -> (T, T)\n",
    "        contrib = torch.triu(contrib, diagonal=0) # (T, T)\n",
    "        contrib = torch.masked_fill(contrib, contrib == 0, float('-inf')) # (T, T)\n",
    "        contrib = torch.softmax(contrib * (HEAD_SIZE ** -0.5), dim=-1) # (T, T)\n",
    "        x = (value.transpose(-2, -1) @ contrib).transpose(-2,-1) # B, (H, T) @ (T, T) -> H, T -> B, T, H\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear1(x)\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if y is not None:\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, x):\n",
    "        logits, _ = self.forward(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        pred = torch.multinomial(probs, 1)\n",
    "\n",
    "        return pred\n",
    "    \n",
    "class TransformerWithAttn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_emb = nn.Embedding(vocab_size, TOK_EMB_DIM)\n",
    "        self.pos_emb = nn.Embedding(CONTEXT_LENGTH, POS_EMB_DIM)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.attn1 = Attention(TOK_EMB_DIM, HEAD_SIZE)\n",
    "        self.linear1 = nn.Linear(CONTEXT_LENGTH * HEAD_SIZE, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, y = None):\n",
    "        # (B, T)\n",
    "        tok_emb = self.tok_emb(x) # (B, T, C)\n",
    "        pos_emb = self.pos_emb(torch.arange(0,CONTEXT_LENGTH)) # (B, T, C)\n",
    "        x = tok_emb + pos_emb # (B, T, C)\n",
    "\n",
    "        x = self.attn1(x)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear1(x)\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if y is not None:\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, x):\n",
    "        logits, _ = self.forward(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        pred = torch.multinomial(probs, 1)\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "class TransformerWithMultiHeadAttn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_emb = nn.Embedding(vocab_size, TOK_EMB_DIM)\n",
    "        self.pos_emb = nn.Embedding(CONTEXT_LENGTH, POS_EMB_DIM)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.attn1 = MultiHeadAttention(num_heads=TOK_EMB_DIM // HEAD_SIZE, embedding_dim=TOK_EMB_DIM, head_size=HEAD_SIZE)\n",
    "        self.linear1 = nn.Linear(CONTEXT_LENGTH * TOK_EMB_DIM, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, y = None):\n",
    "        # (B, T)\n",
    "        tok_emb = self.tok_emb(x) # (B, T, C)\n",
    "        pos_emb = self.pos_emb(torch.arange(0,CONTEXT_LENGTH)) # (B, T, C)\n",
    "        x = tok_emb + pos_emb # (B, T, C)\n",
    "\n",
    "        x = self.attn1(x) # (B, T, H), \n",
    "        # num_heads = TOK_EMB_DIM // HEAD_SIZE\n",
    "        # H * num_heads = tok_emb\n",
    "        # therefore, x (B, T, tok_emb)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear1(x)\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if y is not None:\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, x):\n",
    "        logits, _ = self.forward(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        pred = torch.multinomial(probs, 1)\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_blocks):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, TOK_EMB_DIM)\n",
    "        self.pos_emb = nn.Embedding(CONTEXT_LENGTH, POS_EMB_DIM)\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(TOK_EMB_DIM, NUM_HEADS) for _ in range(num_blocks)])\n",
    "        self.layer_norm = nn.LayerNorm(TOK_EMB_DIM)\n",
    "        self.linear = nn.Linear(CONTEXT_LENGTH * TOK_EMB_DIM, vocab_size)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x, y = None):\n",
    "        # (B, T)\n",
    "        tok_emb = self.tok_emb(x) # (B, T, C)\n",
    "        pos_emb = self.pos_emb(torch.arange(0,CONTEXT_LENGTH).to(device)) # (B, T, C)\n",
    "        x = tok_emb + pos_emb # (B, T, C)\n",
    "\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear(x)\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if y is not None:\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, x):\n",
    "        logits, _ = self.forward(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        pred = torch.multinomial(probs, 1)\n",
    "\n",
    "        return pred\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn1 = MultiHeadAttention(num_heads=num_heads, embedding_dim=embedding_dim, head_size=embedding_dim // num_heads)\n",
    "        # self.attn2 = MultiHeadAttention(num_heads=embedding_dim // head_size, embedding_dim=embedding_dim, head_size=head_size)\n",
    "        self.mlp = MLP(embedding_dim=embedding_dim, expansion_factor=4)\n",
    "        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer_norm1(x)\n",
    "        x = x + self.attn1(out) # (B, T, C)\n",
    "        # x = self.attn2(x) # (B, T, C)\n",
    "        out = self.layer_norm2(x)\n",
    "        x = x + self.mlp(out) # (B, T, C)\n",
    "\n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, embedding_dim, expansion_factor):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(embedding_dim, expansion_factor * embedding_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(embedding_dim * expansion_factor, embedding_dim)\n",
    "        self.dropout = nn.Dropout(DROPOUT_RATE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (B, T, tok_emb)\n",
    "        x = self.linear1(x) # (B, T, 4 * tok_emb)\n",
    "        x = self.relu(x) # (B, T, 4 * tok_emb)\n",
    "        x = self.linear2(x) # (B, T, tok_emb)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, embedding_dim, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attns = nn.ModuleList(Attention(embedding_dim, head_size) for _ in range(num_heads))\n",
    "        self.projection = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.concat([attn(x) for attn in self.attns], dim=-1) \n",
    "        # B, T, (H x num_heads) -> (4 x 8) -> 32 == C (tok_emd)\n",
    "        x = self.projection(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, embedding_dim, head_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(embedding_dim, head_size) # (C, H)\n",
    "        self.key = nn.Linear(embedding_dim, head_size) # (C, H)\n",
    "        self.value = nn.Linear(embedding_dim, head_size) # (C, H)\n",
    "\n",
    "        self.head_size = head_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        query = self.query(x) # B, (T, C) @ (C, H) -> B, (T, H)\n",
    "        key = self.key(x) # B, (T, C) @ (C, H) -> B, (T, H)\n",
    "        value: torch.Tensor = self.value(x) # B, (T, C) @ (C, H) -> B, (T, H)\n",
    "\n",
    "        contrib = query @ key.transpose(-2,-1) # B, (T, H) @ (H, T) -> (T, T)\n",
    "        contrib = torch.triu(contrib, diagonal=0).to(device) # (T, T)\n",
    "        contrib = torch.masked_fill(contrib, contrib == 0, float('-inf')) # (T, T)\n",
    "        contrib = torch.softmax(contrib * (self.head_size ** -0.5), dim=-1) # (T, T)\n",
    "        x = (value.transpose(-2, -1) @ contrib).transpose(-2,-1) # B, (H, T) @ (T, T) -> H, T -> B, T, H\n",
    "\n",
    "        return x\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, head_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(embedding_dim, head_size) # (C, H)\n",
    "        self.key = nn.Linear(embedding_dim, head_size) # (C, H)\n",
    "        self.value = nn.Linear(embedding_dim, head_size) # (C, H)\n",
    "\n",
    "        self.head_size = head_size\n",
    "\n",
    "    def forward(self, x_db, x_client):\n",
    "        query = self.query(x_client) # B, (T, C) @ (C, H) -> B, (T, H)\n",
    "        key = self.key(x_db) # B, (T, C) @ (C, H) -> B, (T, H)\n",
    "        value = self.value(x_db) # B, (T, C) @ (C, H) -> B, (T, H)\n",
    "\n",
    "        contrib = query @ key.transpose(-2,-1) # B, (T, H) @ (H, T) -> (T, T)\n",
    "        contrib = torch.triu(contrib, diagonal=0) # (T, T)\n",
    "        contrib = torch.masked_fill(contrib, contrib == 0, float('-inf')) # (T, T)\n",
    "        contrib = torch.softmax(contrib * (self.head_size ** -0.5), dim=-1) # (T, T)\n",
    "        x = (value.transpose(-2, -1) @ contrib).transpose(-2,-1) # B, (H, T) @ (T, T) -> H, T -> B, T, H\n",
    "\n",
    "        return x\n",
    "    \n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dims, context_length):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(embedding_dims, embedding_dims // 2) # (B, T, C) -> (B, T, C/2)\n",
    "        self.linear2 = nn.Linear(embedding_dims // 2, embedding_dims // 4) # (B, T, C) -> (B, T, C/2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear3 = nn.Linear(embedding_dims // 4 * context_length, (embedding_dims // 4 * context_length) // 4) # (B, T*C/4)\n",
    "        self.linear4 = nn.Linear((embedding_dims // 4 * context_length) // 4, (embedding_dims // 4 * context_length) // 8) # (B, T*C/8)\n",
    "\n",
    "        self.linear5 = nn.Linear((embedding_dims // 4 * context_length) // 8, (embedding_dims // 4 * context_length) // 4) # (B, T*C/4)\n",
    "        self.linear6 = nn.Linear((embedding_dims // 4 * context_length) // 4, embedding_dims // 4 * context_length) # (B, T*C/4) -> (B, T*C)\n",
    "        # view\n",
    "        self.linear7 = nn.Linear(embedding_dims // 4, embedding_dims // 2) # (B, T, C/2) -> (B, T, C)\n",
    "        self.linear8 = nn.Linear(embedding_dims // 2, embedding_dims) # (B, T, C) -> (B, T, C/2)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.linear3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        latents = self.linear4(x)\n",
    "        x = self.relu(latents)\n",
    "\n",
    "        x = self.linear5(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.linear6(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.linear7(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.linear8(x)\n",
    "\n",
    "        return x, latents\n",
    "\n",
    "class HippoNeoCorticalTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.long_term_storage = AutoEncoder(TOK_EMB_DIM, CONTEXT_LENGTH)\n",
    "        self.block = TransformerBlock(TOK_EMB_DIM, HEAD_SIZE)\n",
    "\n",
    "        self.x_attn1 = CrossAttention(TOK_EMB_DIM, HEAD_SIZE)\n",
    "        self.x_attn2 = CrossAttention(TOK_EMB_DIM, HEAD_SIZE)\n",
    "\n",
    "        self.mlp = MLP(TOK_EMB_DIM, 4)\n",
    "\n",
    "    def identity_upsample(self, x):\n",
    "        # TO BE IMPLEMENTED, to upsample latents to fit the dimensions for cross attention\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        recons, latents = self.long_term_storage(x)\n",
    "\n",
    "        x = self.block(x) # (B, T, C)\n",
    "\n",
    "        # identity upsample\n",
    "\n",
    "        x = self.x_attn1(self.identity_upsample(latents), x)\n",
    "        x = self.x_attn2(recons, x)\n",
    "\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "cbff26ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(num_blocks=4).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "4ee364ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.9252649545669556\n",
      "Epoch 1000, Loss: 0.8070555925369263\n",
      "Epoch 2000, Loss: 1.0206773281097412\n",
      "Epoch 3000, Loss: 1.1039702892303467\n",
      "Epoch 4000, Loss: 0.6480349898338318\n",
      "Epoch 5000, Loss: 0.9861669540405273\n",
      "Epoch 6000, Loss: 0.8523643016815186\n",
      "Epoch 7000, Loss: 1.0546671152114868\n",
      "Epoch 8000, Loss: 0.7359679937362671\n",
      "Epoch 9000, Loss: 1.098406434059143\n",
      "Epoch 10000, Loss: 0.7789069414138794\n",
      "Epoch 11000, Loss: 0.7333288192749023\n",
      "Epoch 12000, Loss: 1.2591902017593384\n",
      "Epoch 13000, Loss: 0.92116379737854\n",
      "Epoch 14000, Loss: 0.9404888153076172\n",
      "Epoch 15000, Loss: 0.7107735276222229\n",
      "Epoch 16000, Loss: 0.9913707971572876\n",
      "Epoch 17000, Loss: 0.8289324045181274\n",
      "Epoch 18000, Loss: 0.7959610223770142\n",
      "Epoch 19000, Loss: 0.730248749256134\n",
      "Epoch 20000, Loss: 0.8303301334381104\n",
      "Epoch 21000, Loss: 1.0230498313903809\n",
      "Epoch 22000, Loss: 0.9402694702148438\n",
      "Epoch 23000, Loss: 0.8722807765007019\n",
      "Epoch 24000, Loss: 0.9723681211471558\n",
      "Epoch 25000, Loss: 0.9593546390533447\n",
      "Epoch 26000, Loss: 0.5333551168441772\n",
      "Epoch 27000, Loss: 0.9218989610671997\n",
      "Epoch 28000, Loss: 0.6433735489845276\n",
      "Epoch 29000, Loss: 0.8803757429122925\n",
      "Epoch 30000, Loss: 1.0597951412200928\n",
      "Epoch 31000, Loss: 0.8221799731254578\n",
      "Epoch 32000, Loss: 0.7392900586128235\n",
      "Epoch 33000, Loss: 1.2384932041168213\n",
      "Epoch 34000, Loss: 0.8374558687210083\n",
      "Epoch 35000, Loss: 0.8149813413619995\n",
      "Epoch 36000, Loss: 0.8248133659362793\n",
      "Epoch 37000, Loss: 0.8866488337516785\n",
      "Epoch 38000, Loss: 0.7757385969161987\n",
      "Epoch 39000, Loss: 0.9761897325515747\n",
      "Epoch 40000, Loss: 0.7751336097717285\n",
      "Epoch 41000, Loss: 0.7069345712661743\n",
      "Epoch 42000, Loss: 1.0084112882614136\n",
      "Epoch 43000, Loss: 0.9456561207771301\n",
      "Epoch 44000, Loss: 0.745799720287323\n",
      "Epoch 45000, Loss: 1.1688951253890991\n",
      "Epoch 46000, Loss: 0.8269745111465454\n",
      "Epoch 47000, Loss: 1.177847981452942\n",
      "Epoch 48000, Loss: 0.6782846450805664\n",
      "Epoch 49000, Loss: 0.6063359975814819\n",
      "Epoch 50000, Loss: 0.8400689959526062\n",
      "Epoch 51000, Loss: 0.899041473865509\n",
      "Epoch 52000, Loss: 0.6214005947113037\n",
      "Epoch 53000, Loss: 0.741294264793396\n",
      "Epoch 54000, Loss: 0.8372573852539062\n",
      "Epoch 55000, Loss: 0.6424327492713928\n",
      "Epoch 56000, Loss: 1.318364143371582\n",
      "Epoch 57000, Loss: 0.8122527599334717\n",
      "Epoch 58000, Loss: 1.370230793952942\n",
      "Epoch 59000, Loss: 0.7953591346740723\n",
      "Epoch 60000, Loss: 1.0062768459320068\n",
      "Epoch 61000, Loss: 1.1011024713516235\n",
      "Epoch 62000, Loss: 0.6775928735733032\n",
      "Epoch 63000, Loss: 0.7109504342079163\n",
      "Epoch 64000, Loss: 0.8923631310462952\n",
      "Epoch 65000, Loss: 1.145842432975769\n",
      "Epoch 66000, Loss: 0.9661626815795898\n",
      "Epoch 67000, Loss: 0.7863563299179077\n",
      "Epoch 68000, Loss: 0.8526986837387085\n",
      "Epoch 69000, Loss: 0.4835739731788635\n",
      "Epoch 70000, Loss: 0.912002682685852\n",
      "Epoch 71000, Loss: 0.921338677406311\n",
      "Epoch 72000, Loss: 0.7350283265113831\n",
      "Epoch 73000, Loss: 0.8258683085441589\n",
      "Epoch 74000, Loss: 0.8800303339958191\n",
      "Epoch 75000, Loss: 0.6815765500068665\n",
      "Epoch 76000, Loss: 1.0036976337432861\n",
      "Epoch 77000, Loss: 0.8204940557479858\n",
      "Epoch 78000, Loss: 0.9514743089675903\n",
      "Epoch 79000, Loss: 0.9476722478866577\n",
      "Epoch 80000, Loss: 0.767595648765564\n",
      "Epoch 81000, Loss: 0.9354149103164673\n",
      "Epoch 82000, Loss: 0.7478713989257812\n",
      "Epoch 83000, Loss: 0.6546093225479126\n",
      "Epoch 84000, Loss: 1.0393768548965454\n",
      "Epoch 85000, Loss: 0.6797034740447998\n",
      "Epoch 86000, Loss: 1.0835576057434082\n",
      "Epoch 87000, Loss: 0.7214646935462952\n",
      "Epoch 88000, Loss: 0.9245533347129822\n",
      "Epoch 89000, Loss: 0.648457407951355\n",
      "Epoch 90000, Loss: 0.8754451274871826\n",
      "Epoch 91000, Loss: 1.0700113773345947\n",
      "Epoch 92000, Loss: 0.7412208318710327\n",
      "Epoch 93000, Loss: 0.7561935186386108\n",
      "Epoch 94000, Loss: 0.7964539527893066\n",
      "Epoch 95000, Loss: 0.8002707958221436\n",
      "Epoch 96000, Loss: 0.8127707242965698\n",
      "Epoch 97000, Loss: 1.1739108562469482\n",
      "Epoch 98000, Loss: 0.9330809116363525\n",
      "Epoch 99000, Loss: 0.6061758995056152\n",
      "Epoch 100000, Loss: 0.9808695912361145\n",
      "Epoch 101000, Loss: 0.943674623966217\n",
      "Epoch 102000, Loss: 1.017878770828247\n",
      "Epoch 103000, Loss: 0.7408122420310974\n",
      "Epoch 104000, Loss: 0.6709027290344238\n",
      "Epoch 105000, Loss: 0.635871171951294\n",
      "Epoch 106000, Loss: 0.9422441720962524\n",
      "Epoch 107000, Loss: 0.8635624647140503\n",
      "Epoch 108000, Loss: 0.6825360655784607\n",
      "Epoch 109000, Loss: 0.8939456343650818\n",
      "Epoch 110000, Loss: 0.8308305144309998\n",
      "Epoch 111000, Loss: 1.1128878593444824\n",
      "Epoch 112000, Loss: 0.824630618095398\n",
      "Epoch 113000, Loss: 0.8841440677642822\n",
      "Epoch 114000, Loss: 0.7216477394104004\n",
      "Epoch 115000, Loss: 0.8420886993408203\n",
      "Epoch 116000, Loss: 0.9886072874069214\n",
      "Epoch 117000, Loss: 0.9259063601493835\n",
      "Epoch 118000, Loss: 0.6546692848205566\n",
      "Epoch 119000, Loss: 0.8191092014312744\n",
      "Epoch 120000, Loss: 0.8507419228553772\n",
      "Epoch 121000, Loss: 0.8295334577560425\n",
      "Epoch 122000, Loss: 0.583590567111969\n",
      "Epoch 123000, Loss: 0.833207368850708\n",
      "Epoch 124000, Loss: 0.9093942642211914\n",
      "Epoch 125000, Loss: 0.8005725145339966\n",
      "Epoch 126000, Loss: 0.8359276652336121\n",
      "Epoch 127000, Loss: 1.0088438987731934\n",
      "Epoch 128000, Loss: 0.877342700958252\n",
      "Epoch 129000, Loss: 0.920130729675293\n",
      "Epoch 130000, Loss: 0.8501190543174744\n",
      "Epoch 131000, Loss: 1.2173731327056885\n",
      "Epoch 132000, Loss: 0.7942819595336914\n",
      "Epoch 133000, Loss: 0.7594537734985352\n",
      "Epoch 134000, Loss: 0.9544639587402344\n",
      "Epoch 135000, Loss: 0.6732337474822998\n",
      "Epoch 136000, Loss: 0.6834251880645752\n",
      "Epoch 137000, Loss: 0.939893901348114\n",
      "Epoch 138000, Loss: 0.648659348487854\n",
      "Epoch 139000, Loss: 0.7698524594306946\n",
      "Epoch 140000, Loss: 0.9387980699539185\n",
      "Epoch 141000, Loss: 0.7965424060821533\n",
      "Epoch 142000, Loss: 0.8211351633071899\n",
      "Epoch 143000, Loss: 0.9547103047370911\n",
      "Epoch 144000, Loss: 1.1536017656326294\n",
      "Epoch 145000, Loss: 0.7846311330795288\n",
      "Epoch 146000, Loss: 0.771176278591156\n",
      "Epoch 147000, Loss: 0.5861838459968567\n",
      "Epoch 148000, Loss: 0.929672360420227\n",
      "Epoch 149000, Loss: 0.8570805191993713\n",
      "Epoch 150000, Loss: 1.1477916240692139\n",
      "Epoch 151000, Loss: 0.8292014598846436\n",
      "Epoch 152000, Loss: 0.9577436447143555\n",
      "Epoch 153000, Loss: 0.6646965146064758\n",
      "Epoch 154000, Loss: 0.8609198331832886\n",
      "Epoch 155000, Loss: 0.9682310223579407\n",
      "Epoch 156000, Loss: 0.9828947186470032\n",
      "Epoch 157000, Loss: 0.7787285447120667\n",
      "Epoch 158000, Loss: 0.9461979269981384\n",
      "Epoch 159000, Loss: 0.7522538900375366\n",
      "Epoch 160000, Loss: 0.9076166749000549\n",
      "Epoch 161000, Loss: 0.8327863216400146\n",
      "Epoch 162000, Loss: 0.865818202495575\n",
      "Epoch 163000, Loss: 0.8166869878768921\n",
      "Epoch 164000, Loss: 0.7966856956481934\n",
      "Epoch 165000, Loss: 0.7491376399993896\n",
      "Epoch 166000, Loss: 0.924485445022583\n",
      "Epoch 167000, Loss: 1.0802278518676758\n",
      "Epoch 168000, Loss: 0.6164102554321289\n",
      "Epoch 169000, Loss: 0.6249914765357971\n",
      "Epoch 170000, Loss: 0.8069969415664673\n",
      "Epoch 171000, Loss: 1.0313738584518433\n",
      "Epoch 172000, Loss: 0.8312450647354126\n",
      "Epoch 173000, Loss: 0.7996658086776733\n",
      "Epoch 174000, Loss: 0.8336920738220215\n",
      "Epoch 175000, Loss: 1.1683907508850098\n",
      "Epoch 176000, Loss: 0.8425442576408386\n",
      "Epoch 177000, Loss: 1.0336573123931885\n",
      "Epoch 178000, Loss: 1.1573290824890137\n",
      "Epoch 179000, Loss: 0.8034498691558838\n",
      "Epoch 180000, Loss: 0.9477486610412598\n",
      "Epoch 181000, Loss: 0.8343207240104675\n",
      "Epoch 182000, Loss: 0.6500415802001953\n",
      "Epoch 183000, Loss: 0.5163406133651733\n",
      "Epoch 184000, Loss: 0.8370617628097534\n",
      "Epoch 185000, Loss: 0.7755045890808105\n",
      "Epoch 186000, Loss: 1.1133953332901\n",
      "Epoch 187000, Loss: 0.8081672787666321\n",
      "Epoch 188000, Loss: 0.7015699148178101\n",
      "Epoch 189000, Loss: 0.8273048400878906\n",
      "Epoch 190000, Loss: 0.8226852416992188\n",
      "Epoch 191000, Loss: 0.8831039071083069\n",
      "Epoch 192000, Loss: 0.9172112345695496\n",
      "Epoch 193000, Loss: 0.9729959964752197\n",
      "Epoch 194000, Loss: 0.9378604292869568\n",
      "Epoch 195000, Loss: 0.8488476276397705\n",
      "Epoch 196000, Loss: 0.8404691219329834\n",
      "Epoch 197000, Loss: 0.7116466760635376\n",
      "Epoch 198000, Loss: 0.7427707314491272\n",
      "Epoch 199000, Loss: 1.0289585590362549\n",
      "Epoch 200000, Loss: 0.642659068107605\n",
      "Epoch 201000, Loss: 1.0309290885925293\n",
      "Epoch 202000, Loss: 0.7255617380142212\n",
      "Epoch 203000, Loss: 0.795066773891449\n",
      "Epoch 204000, Loss: 0.8546545505523682\n",
      "Epoch 205000, Loss: 1.248548984527588\n",
      "Epoch 206000, Loss: 1.2513500452041626\n",
      "Epoch 207000, Loss: 0.958626389503479\n",
      "Epoch 208000, Loss: 0.7986048460006714\n",
      "Epoch 209000, Loss: 0.919891357421875\n",
      "Epoch 210000, Loss: 0.7666825652122498\n",
      "Epoch 211000, Loss: 0.8081306219100952\n",
      "Epoch 212000, Loss: 0.9044665098190308\n",
      "Epoch 213000, Loss: 0.8103084564208984\n",
      "Epoch 214000, Loss: 0.6777154207229614\n",
      "Epoch 215000, Loss: 0.9823380708694458\n",
      "Epoch 216000, Loss: 0.7864447832107544\n",
      "Epoch 217000, Loss: 0.7881994247436523\n",
      "Epoch 218000, Loss: 0.6985182166099548\n",
      "Epoch 219000, Loss: 0.8938729166984558\n",
      "Epoch 220000, Loss: 0.8927202224731445\n",
      "Epoch 221000, Loss: 1.1476926803588867\n",
      "Epoch 222000, Loss: 0.696898341178894\n",
      "Epoch 223000, Loss: 0.7568329572677612\n",
      "Epoch 224000, Loss: 0.8275573253631592\n",
      "Epoch 225000, Loss: 0.8007599115371704\n",
      "Epoch 226000, Loss: 0.7270475625991821\n",
      "Epoch 227000, Loss: 0.6775230169296265\n",
      "Epoch 228000, Loss: 0.9909791946411133\n",
      "Epoch 229000, Loss: 0.8371977806091309\n",
      "Epoch 230000, Loss: 0.9780806303024292\n",
      "Epoch 231000, Loss: 1.1118254661560059\n",
      "Epoch 232000, Loss: 0.879244863986969\n",
      "Epoch 233000, Loss: 1.0753860473632812\n",
      "Epoch 234000, Loss: 0.9645857810974121\n",
      "Epoch 235000, Loss: 0.8399134874343872\n",
      "Epoch 236000, Loss: 0.8103964328765869\n",
      "Epoch 237000, Loss: 1.1144626140594482\n",
      "Epoch 238000, Loss: 0.9212154150009155\n",
      "Epoch 239000, Loss: 0.845617413520813\n",
      "Epoch 240000, Loss: 0.8615949153900146\n",
      "Epoch 241000, Loss: 0.7230879664421082\n",
      "Epoch 242000, Loss: 0.678959310054779\n",
      "Epoch 243000, Loss: 0.8175516128540039\n",
      "Epoch 244000, Loss: 0.7113687992095947\n",
      "Epoch 245000, Loss: 0.7409842610359192\n",
      "Epoch 246000, Loss: 0.9018296599388123\n",
      "Epoch 247000, Loss: 0.7420526742935181\n",
      "Epoch 248000, Loss: 0.6138378381729126\n",
      "Epoch 249000, Loss: 0.9035718441009521\n",
      "Epoch 250000, Loss: 1.1327970027923584\n",
      "Epoch 251000, Loss: 0.8999526500701904\n",
      "Epoch 252000, Loss: 0.7214446663856506\n",
      "Epoch 253000, Loss: 0.8048425912857056\n",
      "Epoch 254000, Loss: 1.109058141708374\n",
      "Epoch 255000, Loss: 0.9596880674362183\n",
      "Epoch 256000, Loss: 0.9454258680343628\n",
      "Epoch 257000, Loss: 0.9211944341659546\n",
      "Epoch 258000, Loss: 0.7730847597122192\n",
      "Epoch 259000, Loss: 0.746383547782898\n",
      "Epoch 260000, Loss: 1.0407941341400146\n",
      "Epoch 261000, Loss: 0.5938275456428528\n",
      "Epoch 262000, Loss: 0.6267321705818176\n",
      "Epoch 263000, Loss: 0.9699693918228149\n",
      "Epoch 264000, Loss: 1.0060234069824219\n",
      "Epoch 265000, Loss: 0.8512541651725769\n",
      "Epoch 266000, Loss: 0.7623622417449951\n",
      "Epoch 267000, Loss: 0.9997659921646118\n",
      "Epoch 268000, Loss: 0.9248993396759033\n",
      "Epoch 269000, Loss: 0.76871657371521\n",
      "Epoch 270000, Loss: 0.9532018303871155\n",
      "Epoch 271000, Loss: 0.6968640089035034\n",
      "Epoch 272000, Loss: 0.7479965090751648\n",
      "Epoch 273000, Loss: 0.8379522562026978\n",
      "Epoch 274000, Loss: 0.7914925813674927\n",
      "Epoch 275000, Loss: 0.7342411875724792\n",
      "Epoch 276000, Loss: 0.807870090007782\n",
      "Epoch 277000, Loss: 0.9824701547622681\n",
      "Epoch 278000, Loss: 0.8511413931846619\n",
      "Epoch 279000, Loss: 0.7894644737243652\n",
      "Epoch 280000, Loss: 0.926048994064331\n",
      "Epoch 281000, Loss: 1.004373550415039\n",
      "Epoch 282000, Loss: 0.9769701957702637\n",
      "Epoch 283000, Loss: 0.7955070734024048\n",
      "Epoch 284000, Loss: 0.7830560803413391\n",
      "Epoch 285000, Loss: 1.2621163129806519\n",
      "Epoch 286000, Loss: 0.7874926328659058\n",
      "Epoch 287000, Loss: 0.8755033016204834\n",
      "Epoch 288000, Loss: 1.0189061164855957\n",
      "Epoch 289000, Loss: 0.759434700012207\n",
      "Epoch 290000, Loss: 0.7539253830909729\n",
      "Epoch 291000, Loss: 1.180766224861145\n",
      "Epoch 292000, Loss: 0.6219344735145569\n",
      "Epoch 293000, Loss: 1.0726745128631592\n",
      "Epoch 294000, Loss: 0.7404424548149109\n",
      "Epoch 295000, Loss: 1.062308669090271\n",
      "Epoch 296000, Loss: 0.7292599678039551\n",
      "Epoch 297000, Loss: 0.7676151394844055\n",
      "Epoch 298000, Loss: 0.8052751421928406\n",
      "Epoch 299000, Loss: 0.8485055565834045\n",
      "Epoch 300000, Loss: 0.8772816061973572\n",
      "Epoch 301000, Loss: 0.6646389961242676\n",
      "Epoch 302000, Loss: 1.2090370655059814\n",
      "Epoch 303000, Loss: 0.8106808662414551\n",
      "Epoch 304000, Loss: 0.9340863823890686\n",
      "Epoch 305000, Loss: 0.7558180093765259\n",
      "Epoch 306000, Loss: 0.773859441280365\n",
      "Epoch 307000, Loss: 0.8385279774665833\n",
      "Epoch 308000, Loss: 0.7093508243560791\n",
      "Epoch 309000, Loss: 0.9564542174339294\n",
      "Epoch 310000, Loss: 0.7959203124046326\n",
      "Epoch 311000, Loss: 1.0057274103164673\n",
      "Epoch 312000, Loss: 0.6439372897148132\n",
      "Epoch 313000, Loss: 0.7883580923080444\n",
      "Epoch 314000, Loss: 1.1574711799621582\n",
      "Epoch 315000, Loss: 0.9675392508506775\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[378]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m xs = X_tr[batch_ix].to(device)\n\u001b[32m      6\u001b[39m y = Y_tr[batch_ix].to(device)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m logits, loss = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     10\u001b[39m loss.backward()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[354]\u001b[39m\u001b[32m, line 153\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, x, y)\u001b[39m\n\u001b[32m    150\u001b[39m pos_emb = \u001b[38;5;28mself\u001b[39m.pos_emb(torch.arange(\u001b[32m0\u001b[39m,CONTEXT_LENGTH).to(device)) \u001b[38;5;66;03m# (B, T, C)\u001b[39;00m\n\u001b[32m    151\u001b[39m x = tok_emb + pos_emb \u001b[38;5;66;03m# (B, T, C)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer_norm(x)\n\u001b[32m    156\u001b[39m x = \u001b[38;5;28mself\u001b[39m.flatten(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[354]\u001b[39m\u001b[32m, line 186\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    185\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.layer_norm1(x)\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     x = x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, T, C)\u001b[39;00m\n\u001b[32m    187\u001b[39m     \u001b[38;5;66;03m# x = self.attn2(x) # (B, T, C)\u001b[39;00m\n\u001b[32m    188\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.layer_norm2(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[354]\u001b[39m\u001b[32m, line 221\u001b[39m, in \u001b[36mMultiHeadAttention.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     x = torch.concat([\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m attn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.attns], dim=-\u001b[32m1\u001b[39m) \n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# B, T, (H x num_heads) -> (4 x 8) -> 32 == C (tok_emd)\u001b[39;00m\n\u001b[32m    223\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.projection(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[354]\u001b[39m\u001b[32m, line 239\u001b[39m, in \u001b[36mAttention.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    238\u001b[39m     query = \u001b[38;5;28mself\u001b[39m.query(x) \u001b[38;5;66;03m# B, (T, C) @ (C, H) -> B, (T, H)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m     key = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# B, (T, C) @ (C, H) -> B, (T, H)\u001b[39;00m\n\u001b[32m    240\u001b[39m     value: torch.Tensor = \u001b[38;5;28mself\u001b[39m.value(x) \u001b[38;5;66;03m# B, (T, C) @ (C, H) -> B, (T, H)\u001b[39;00m\n\u001b[32m    242\u001b[39m     contrib = query @ key.transpose(-\u001b[32m2\u001b[39m,-\u001b[32m1\u001b[39m) \u001b[38;5;66;03m# B, (T, H) @ (H, T) -> (T, T)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/nn/modules/linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# for ep in range(1000):\n",
    "model.train()\n",
    "for ep in range(500_000):\n",
    "    batch_ix = torch.randint(0, X_tr.shape[0], (64,))\n",
    "    xs = X_tr[batch_ix].to(device)\n",
    "    y = Y_tr[batch_ix].to(device)\n",
    "\n",
    "    logits, loss = model.forward(xs, y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    # log every 1000 epochs\n",
    "    if ep % 1000 == 0:\n",
    "        print(f\"Epoch {ep}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "d8569bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to transformer_model_v3.pt\n",
      "Model loaded from transformer_model_v2.pt\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), 'transformer_model_v3.pt')\n",
    "print(\"Model saved to transformer_model_v3.pt\")\n",
    "\n",
    "# load the model\n",
    "loaded_model = Transformer(num_blocks=4).to(device)\n",
    "loaded_model.load_state_dict(torch.load('transformer_model.pt', map_location=device))\n",
    "loaded_model.eval()\n",
    "print(\"Model loaded from transformer_model_v2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "5e04b9d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[380]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m xs = X_val[i].unsqueeze(\u001b[32m0\u001b[39m).to(device)\n\u001b[32m      6\u001b[39m y = Y_val[i].unsqueeze(\u001b[32m0\u001b[39m).to(device)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m logits, loss = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m val_losses.append(loss.item())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[354]\u001b[39m\u001b[32m, line 153\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, x, y)\u001b[39m\n\u001b[32m    150\u001b[39m pos_emb = \u001b[38;5;28mself\u001b[39m.pos_emb(torch.arange(\u001b[32m0\u001b[39m,CONTEXT_LENGTH).to(device)) \u001b[38;5;66;03m# (B, T, C)\u001b[39;00m\n\u001b[32m    151\u001b[39m x = tok_emb + pos_emb \u001b[38;5;66;03m# (B, T, C)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer_norm(x)\n\u001b[32m    156\u001b[39m x = \u001b[38;5;28mself\u001b[39m.flatten(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[354]\u001b[39m\u001b[32m, line 186\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    185\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.layer_norm1(x)\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     x = x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, T, C)\u001b[39;00m\n\u001b[32m    187\u001b[39m     \u001b[38;5;66;03m# x = self.attn2(x) # (B, T, C)\u001b[39;00m\n\u001b[32m    188\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.layer_norm2(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[354]\u001b[39m\u001b[32m, line 221\u001b[39m, in \u001b[36mMultiHeadAttention.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     x = torch.concat([\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m attn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.attns], dim=-\u001b[32m1\u001b[39m) \n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# B, T, (H x num_heads) -> (4 x 8) -> 32 == C (tok_emd)\u001b[39;00m\n\u001b[32m    223\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.projection(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[354]\u001b[39m\u001b[32m, line 239\u001b[39m, in \u001b[36mAttention.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    238\u001b[39m     query = \u001b[38;5;28mself\u001b[39m.query(x) \u001b[38;5;66;03m# B, (T, C) @ (C, H) -> B, (T, H)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m     key = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# B, (T, C) @ (C, H) -> B, (T, H)\u001b[39;00m\n\u001b[32m    240\u001b[39m     value: torch.Tensor = \u001b[38;5;28mself\u001b[39m.value(x) \u001b[38;5;66;03m# B, (T, C) @ (C, H) -> B, (T, H)\u001b[39;00m\n\u001b[32m    242\u001b[39m     contrib = query @ key.transpose(-\u001b[32m2\u001b[39m,-\u001b[32m1\u001b[39m) \u001b[38;5;66;03m# B, (T, H) @ (H, T) -> (T, T)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/kingscrosshacklab/.venv/lib/python3.13/site-packages/torch/nn/modules/linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    for i in range(len(X_val)):\n",
    "        xs = X_val[i].unsqueeze(0).to(device)\n",
    "        y = Y_val[i].unsqueeze(0).to(device)\n",
    "\n",
    "        logits, loss = model.forward(xs, y)\n",
    "        val_losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "339a0d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x134dfbc50>]"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(torch.tensor(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "35122c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x147101e50>]"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGdCAYAAAAxCSikAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAP8dJREFUeJzt3Qd4VFX6x/E3hSQQUgiQBqE36b2jsCKI/FHYXRXWlbKgq4IrixXXtrsqrm3VXRYsi9hRV0EFpUiVLk0p0gMJkISahARInf/znjBjBgZmgsncSfL9PM912p3JzZ0h8/Oc95zjZ7PZbAIAAODD/K0+AAAAAHcILAAAwOcRWAAAgM8jsAAAAJ9HYAEAAD6PwAIAAHwegQUAAPg8AgsAAPB5gVIBFBYWypEjRyQsLEz8/PysPhwAAOABnbv29OnTEh8fL/7+/hU/sGhYSUhIsPowAADAFUhOTpa6detW/MCiLSv2Xzg8PNzqwwEAAB7IzMw0DQ727/EKH1js3UAaVggsAACUL56Uc1B0CwAAfB6BBQAA+DwCCwAA8HkEFgAA4PMILAAAwOcRWAAAgM8jsAAAAJ9HYAEAAD6PwAIAAHwegQUAAPg8AgsAAPB5BBYAAODzKsTih2Ulr6BQnpn3k7n+yKAWElIlwOpDAgCgUqKF5TIKbTaZufqA2TS8AAAAaxBYLndyii13XWjzxtsBAABcIbB4GFhsNhILAABWIbBcxs9xhRYWAACsRGC5jGINLLSwAABgIQLLZfj5+TlCCzUsAABYh8Di7gSdTyzUsAAAYB0Cixv2XiFaWAAAsA6BxdMWFmGUEAAAViGwuEENCwAA1iOwuDtB5xNLIX1CAABYhsDiYQsL88YBAGAdAou7E0QNCwAAliOwuEENCwAA5SywTJkyRbp06SJhYWESHR0tQ4cOlV27dl32OTNnzjw/AdvPW0hIiNM+OsfJE088IXFxcVK1alXp37+/7NmzR3yqhoU+IQAAykdgWb58uYwfP17Wrl0rixYtkry8PBkwYIBkZ2df9nnh4eGSkpLi2A4ePOj0+PPPPy+vvfaaTJ8+XdatWyehoaEycOBAOXfunPhODQvDmgEAsEpgSXaeP3/+Ra0n2tKyceNGufrqqy/5PG1ViY2NdfmYBoFXXnlFHnvsMbnpppvMfe+++67ExMTInDlzZPjw4eIbM91aehgAAFRqv6iGJSMjw1xGRUVddr+srCypX7++JCQkmFCyfft2x2OJiYmSmppquoHsIiIipFu3brJmzRqXr5eTkyOZmZlOW1nxZy0hAADKb2ApLCyUiRMnSq9evaR169aX3K958+YyY8YM+eKLL+T99983z+vZs6ccOnTIPK5hRWmLSnF62/6Yq1oaDTX2TYNQ2aGGBQCAchtYtJZl27ZtMmvWrMvu16NHDxk5cqS0b99errnmGvn888+ldu3a8vrrr1/pj5bJkyeb1h37lpycLGXfwkKfEAAA5aKGxW7ChAkyd+5cWbFihdStW7dEz61SpYp06NBB9u7da27ba1vS0tLMKCE7va0hx5Xg4GCzeQM1LAAAlLMWFi2Q1bAye/ZsWbJkiTRs2LDEP7CgoEC2bt3qCCf6GhpaFi9e7NhHa1J0tJC2zljN3sJCAwsAAOWkhUW7gT788ENTj6JzsdhrTLSOROdPUdr9U6dOHVNnov72t79J9+7dpUmTJpKeni4vvPCCGdY8btw4xwgirYV5+umnpWnTpibAPP744xIfH2/mebGaHp+iSwgAgHISWKZNm2Yu+/bt63T/22+/LaNHjzbXk5KSxN//54abU6dOyR133GHCTY0aNaRTp06yevVqadmypWOfhx56yMzlcuedd5pQ07t3bzOE+sIJ5qyd6ZYaFgAArOJnqwAzomkXkrbyaAGuTlJXmq5+fqkknTwjn9/TUzrWq1Gqrw0AQGWWWYLvb9YS8riGpdznOgAAyi0Ci8c1LN54OwAAgCsEFk9rWEgsAABYhsDi6VpC3ng3AACASwQWN5jpFgAA6xFY3PA7v5YQNbcAAFiHwOIG87AAAGA9AounNSwUsQAAYBkCixu0sAAAYD0Ci7sTRAsLAACWI7C4O0GsJQQAgOUILB7OdEsNCwAA1iGwuEENCwAA1iOwuDtBrCUEAIDlCCzuThCrNQMAYDkCi6c1LN54NwAAgEsEFjfON7BIIVW3AABYhsDi7gRRwwIAgOUILO5O0PkzZKOFBQAAyxBY3J0g5mEBAMByBBYPUcMCAIB1CCzuThA1LAAAWI7A4u4EsZYQAACWI7C4O0H2ufmZiAUAAMsQWNxgLSEAAKxHYPFwpttCWlgAALAMgcXdCXL0CJFYAACwCoHFDb/zk/PTwgIAgHUILO5OEDPdAgBgOQKLpzUsNLEAAGAZAounU/N7490AAAAuEVjcOF9zSw0LAAAWIrB4OkqI1ZoBALAMgcXdCXLMw0KnEAAAViGweFh0S14BAMA6BBaPp+b3wrsBAABcIrC4wWrNAACUs8AyZcoU6dKli4SFhUl0dLQMHTpUdu3addnnvPnmm9KnTx+pUaOG2fr37y/r16932mf06NGm66X4dv3114tPDWumTwgAgPIRWJYvXy7jx4+XtWvXyqJFiyQvL08GDBgg2dnZl3zOsmXLZMSIEbJ06VJZs2aNJCQkmOccPnzYaT8NKCkpKY7to48+El9ADQsAANYLLMnO8+fPd7o9c+ZM09KyceNGufrqq10+54MPPnC6/dZbb8lnn30mixcvlpEjRzruDw4OltjYWPE11LAAAFDOa1gyMjLMZVRUlMfPOXPmjGmZufA52hKj4ad58+Zy9913y4kTJy75Gjk5OZKZmem0lRVqWAAAKMeBpbCwUCZOnCi9evWS1q1be/y8hx9+WOLj400tS/HuoHfffde0uvzjH/8wXU+DBg2SgoKCS9bSREREODbtZior1LAAAFDOuoSK01qWbdu2ycqVKz1+znPPPSezZs0yrSkhISGO+4cPH+643qZNG2nbtq00btzY7Hfttdde9DqTJ0+WSZMmOW5rC0tZhRbWEgIAoJy2sEyYMEHmzp1rCmnr1q3r0XNefPFFE1gWLlxoAsnlNGrUSGrVqiV79+51+bjWu4SHhzttZY2ZbgEAKCctLDq0995775XZs2eb1o+GDRt69Lznn39ennnmGVmwYIF07tzZ7f6HDh0yNSxxcXHiO1PzW30kAABUXv4l7QZ6//335cMPPzRzsaSmpprt7Nmzjn105I922dhpTcrjjz8uM2bMkAYNGjiek5WVZR7XywcffNAMlT5w4ICpY7npppukSZMmMnDgQLEaRbcAAJSzwDJt2jQzMqhv376m9cO+ffzxx459kpKSzDwqxZ+Tm5srv/3tb52eo11EKiAgQH788Ue58cYbpVmzZjJ27Fjp1KmTfPfdd6brx1eGNQstLAAAlJ8uIXe0q6g4bTW5nKpVq5quIl/Fas0AAFiPtYQ8nOmWGhYAAKxDYHF3ghyrNdMnBACAVQgsHtawkFcAALAOgcXdCWK1ZgAALEdgcYMaFgAArEdgcXeCqGEBAMByBBY3/KQosVByCwCAdQgs7k6Qo+iWyAIAgFUILO5O0PnEUljojbcDAAC4QmDxcFgz87AAAGAdAosb1LAAAGA9Aou7E0QLCwAAliOweDxxnDfeDgAA4AqBxQ1qWAAAsB6BxcOZbmlhAQDAOgQWdyeIGhYAACxHYHF3gmhhAQDAcgQWN6hhAQDAegQWN6hhAQDAegQWdyeIGhYAACxHYHF3gs73CRUyDwsAAJYhsLhxvoGF1ZoBALAQgcXTUULeeDcAAIBLBBY3GCUEAID1CCzuThA1LAAAWI7A4mELi425+QEAsAyBxd0JYqZbAAAsR2BxgxoWAACsR2DxuIaFcUIAAFiFwOJxDYsX3g0AAOASgcUNalgAALAegcXdCWItIQAALEdgcYsaFgAArEZgcXeC7DUsXngzAACAawQWN5jpFgAA6xFY3J2g82eImW4BALAOgcUNP2pYAAAoX4FlypQp0qVLFwkLC5Po6GgZOnSo7Nq1y+3zPv30U2nRooWEhIRImzZt5Ouvv3Z6XFsvnnjiCYmLi5OqVatK//79Zc+ePeILmIcFAIByFliWL18u48ePl7Vr18qiRYskLy9PBgwYINnZ2Zd8zurVq2XEiBEyduxY2bx5swk5um3bts2xz/PPPy+vvfaaTJ8+XdatWyehoaEycOBAOXfunPhKDUtBIWW3AABYxc/2C4ozjh07ZlpaNMhcffXVLve59dZbTaCZO3eu477u3btL+/btTUDRHx8fHy/333+/PPDAA+bxjIwMiYmJkZkzZ8rw4cPdHkdmZqZERESY54WHh0tpWr3vuPzuzXXSNLq6LJp0Tam+NgAAlVlmCb6/f1ENi/4AFRUVdcl91qxZY7p4itPWE71fJSYmSmpqqtM+evDdunVz7HOhnJwc80sW38pKUEDRKcotKCyznwEAAKRsAkthYaFMnDhRevXqJa1bt77kfhpGtLWkOL2t99sft993qX1c1dJoqLFvCQkJUlaqnA8sefkEFgAAyl1g0VoWrUOZNWuWeNvkyZNN6459S05OLrOfFRRob2GhhgUAAKsEXsmTJkyYYGpSVqxYIXXr1r3svrGxsZKWluZ0n97W++2P2+/TUULF99E6F1eCg4PN5g2OFha6hAAAKB8tLFogq2Fl9uzZsmTJEmnYsKHb5/To0UMWL17sdJ+OMNL7lb6Ghpbi+2hNio4Wsu9jJUcNC11CAACUjxYW7Qb68MMP5YsvvjBzsdhrTLSOROdPUSNHjpQ6deqYOhN13333yTXXXCMvvfSSDB482HQhbdiwQd544w3zuJ+fn6mFefrpp6Vp06YmwDz++ONm5JAOf7aavUuIFhYAAMpJYJk2bZq57Nu3r9P9b7/9towePdpcT0pKEn/7fPYi0rNnTxNyHnvsMXn00UdNKJkzZ45Toe5DDz1khj7feeedkp6eLr1795b58+ebieasViWgaB6W/EKbFBbaxN++GiIAACgf87D4irKchyXzXJ60fWqhub7r6eslODCgVF8fAIDKKtNb87BUBvYaFkUdCwAA1iCweDhKSOUxtBkAAEsQWNwI8Pczm6LwFgAAaxBYSlB4S5cQAADWILB4gPWEAACwFoHFA8zFAgCAtQgsJVoAsdyPAAcAoFwisJRoAcSCsn4/AACACwSWErSw5NLCAgCAJQgsHmDFZgAArEVg8UDQ+WHNzMMCAIA1CCwlqWHJLyzr9wMAALhAYClJDUsBgQUAACsQWEpUw8KwZgAArEBg8QATxwEAYC0CS0mm5qeGBQAASxBYSrD4IaOEAACwBoHFAxTdAgBgLQKLBxjWDACAtQgsHmCmWwAArEVgKdEoIYY1AwBgBQJLCUYJ5eSxWjMAAFYgsHigWnCAuczOJbAAAGAFAosHqgcHmsvsnPyyfj8AAIALBBYPhAYVBZYsAgsAAJYgsHigegiBBQAAKxFYPECXEAAA1iKweCDUUcNC0S0AAFYgsJSghYUaFgAArEFgKWFgsdmYPA4AAG8jsHgg9Pw8LAWFNsnJLyzr9wQAAFyAwOKB0PPDmhXdQgAAeB+BxZOT5O8noUFFrSxZ55g8DgAAbyOwlHCkEC0sAAB4H4GlhJPHMT0/AADeR2DxEEObAQCwDoHFQ6GsJwQAgGUILCXuEmK2WwAAfD6wrFixQoYMGSLx8fHi5+cnc+bMuez+o0ePNvtduLVq1cqxz1NPPXXR4y1atBDf7BLKs/pQAACodEocWLKzs6Vdu3YydepUj/Z/9dVXJSUlxbElJydLVFSU3HzzzU77aYApvt/KlSvFFyePy6KFBQAAr/t5RjQPDRo0yGyeioiIMJudtsicOnVKxowZ43wggYESGxsrvqp6cBVzySghAAAqQQ3Lf//7X+nfv7/Ur1/f6f49e/aYbqZGjRrJbbfdJklJSZd8jZycHMnMzHTaylp1ewsLE8cBAFCxA8uRI0fkm2++kXHjxjnd361bN5k5c6bMnz9fpk2bJomJidKnTx85ffq0y9eZMmWKo+VGt4SEBO9NHJfLTLcAAFTowPLOO+9IZGSkDB061Ol+7WLSmpa2bdvKwIED5euvv5b09HT55JNPXL7O5MmTJSMjw7FpXYy3im7pEgIAoBzUsFwpm80mM2bMkNtvv12CgoIuu6+GmmbNmsnevXtdPh4cHGw2S0YJ0SUEAEDFbWFZvny5CSBjx451u29WVpbs27dP4uLixFewlhAAAOUosGiY2LJli9mU1pvodXuRrHbXjBw50mWxrdaqtG7d+qLHHnjgARNoDhw4IKtXr5Zhw4ZJQECAjBgxQnxu4jhqWAAA8P0uoQ0bNki/fv0ctydNmmQuR40aZQpndQ6VC0f4aJ3JZ599ZuZkceXQoUMmnJw4cUJq164tvXv3lrVr15rrvoIuIQAAylFg6du3r6lHuRQNLRfSkTxnzpy55HNmzZolvs7eJcTU/AAAeB9rCZWwhSW3oFBy8llPCAAAbyKwlDCwKEYKAQDgXQQWDwX4+0n4+cLbU2dYABEAAG8isJRAVGjR/DEns3PL6v0AAAAuEFhKgMACAIA1CCwlQGABAMAaBJYrCCynztAlBACANxFYSqDG+cByIovAAgCANxFYSqCmo+g2p6zeDwAA4AKBpQSiQotWiD7JsGYAALyKwFICUaFVzCUtLAAAeBeBpQSiw0LMZVomXUIAAHgTgaUE4iOrmstjp3NYTwgAAC8isJRAjWpVJKRK0SlLzThXVu8JAAC4AIGlBPz8/CQ+oqiV5XD62ZI8FQAA/AIElivsFkpJp4UFAABvIbCUUHxkUeHtEVpYAADwGgLLFbawHMmgSwgAAG8hsFxhYDlMlxAAAF5DYCmhOvYWFrqEAADwGgLLlXYJpZ8Vm81WFu8JAAC4AIGlhOIiiopuz+QWSMbZvJI+HQAAXAECSwmFVAmQWtWLVm1mLhYAALyDwPKLuoWYiwUAAG8gsFyBmHD7IogEFgAAvIHAcgWiw4LN5dHTrNoMAIA3EFiuQHRYUQvLsdO0sAAA4A0ElisQHV7UwpKWSQsLAADeQGD5RV1CtLAAAOANBJZf0CV0lBYWAAC8gsDyC7qEjmflSEEhs90CAFDWCCxXoGZokPj7iWhWOZFFHQsAAGWNwHIFAgP8Jfb8XCyHWAQRAIAyR2C5QnWjqpnL5JNnSvP9AAAALhBYrlDdGkXT8x86dfZKXwIAAHiIwHKFEmrQwgIAgLcQWK5QwvkuIVpYAADwwcCyYsUKGTJkiMTHx4ufn5/MmTPnsvsvW7bM7Hfhlpqa6rTf1KlTpUGDBhISEiLdunWT9evXiy9rWKsosGxKOiX7jmVZfTgAAFRoJQ4s2dnZ0q5dOxMwSmLXrl2SkpLi2KKjox2PffzxxzJp0iR58sknZdOmTeb1Bw4cKEePHhVf1T6hhnRvFCVncgtkxspEqw8HAIAKLbCkTxg0aJDZSkoDSmRkpMvHXn75ZbnjjjtkzJgx5vb06dNl3rx5MmPGDHnkkUfEFwX4+8mwDnVk7f6TcoShzQAAVIwalvbt20tcXJxcd911smrVKsf9ubm5snHjRunfv//PB+Xvb26vWbPG5Wvl5ORIZmam02aF2ufXFDrG5HEAAJTvwKIhRVtMPvvsM7MlJCRI3759TdePOn78uBQUFEhMTIzT8/T2hXUudlOmTJGIiAjHpq9phdrViyaPO3aa2W4BAPCpLqGSat68udnsevbsKfv27ZN//vOf8t57713Ra06ePNnUvNhpC4sVoaVWWJC5PJGVK4WFNvHX+foBAED5CyyudO3aVVauXGmu16pVSwICAiQtLc1pH70dGxvr8vnBwcFms1rN0KJjyC+0SfrZPIkKLQowAACgAszDsmXLFtNVpIKCgqRTp06yePFix+OFhYXmdo8ePcSXBQX6S41qVcx1uoUAAPChFpasrCzZu3ev43ZiYqIJIFFRUVKvXj3TXXP48GF59913zeOvvPKKNGzYUFq1aiXnzp2Tt956S5YsWSILFy50vIZ274waNUo6d+5sWl/0OTp82j5qyJfVqh4sp87kyfGsHGkuYVYfDgAAFVKJA8uGDRukX79+jtv2WhINHDNnzjRzrCQlJTmNArr//vtNiKlWrZq0bdtWvv32W6fXuPXWW+XYsWPyxBNPmEJbHVE0f/78iwpxfZGOFNpzNEsOs6YQAABlxs9ms9mknNOiWx0tlJGRIeHh4V792c99s1OmL98nv+5YR16+pb1XfzYAAJXl+5u1hH6hPk1rmcuVe45LBch+AAD4JALLL9Spfg1TfHv0dI4knTxTOu8KAABwQmD5hUKqBEjDmqHmeuLx7F/6cgAAwAUCSymoX7No5eaDJ2hhAQCgLBBYSkGDWkUtLAdO0MICAEBZILCUAlpYAAAoWwSWUtDgfA3L9iMZci6voDReEgAAFENgKQUd6kWaCeTSMnNkxqrE0nhJAABQDIGlFFQLCpQ/929mrn+7w3kRRwAA8MsRWEpJryY1zeW2w5l0CwEAUMoILKWkXlQ1sxBibkGhbDucUVovCwAACCylx8/PTzrVjzTXNxw8xYcLAIBSRAtLKepcP8pcbiSwAABQqggspahj/RrmctPBUyyECABAKSKwlKLWdcLNQognsnNl37Gs0nxpAAAqNQJLKQoODJCuDYq6hZbvPl6aLw0AQKVGYCll1zSrbS6X7Tpa2i8NAEClRWApZf1aRJvLNftOyPGsnNJ+eQAAKiUCSylrEl1d2tWNkPxCm8zedLi0Xx4AgEqJwFIGhnWoYy5X7qWOBQCA0kBgKQOt6kSYy71HGSkEAEBpILCUgSa1q5vLw+lnJTsnvyx+BAAAlQqBpQzUCA2SWtWDzHXmYwEA4JcjsJRh8a3amXq6rH4EAACVBoGljHSoVzRN//Ldx6Sw0FZWPwYAgEqBwFJG+p6fQG7ejyky+F8rJTXjXFn9KAAAKjwCSxkuhBhZrYq5/lNKpvxj/s6y+lEAAFR4BJYyUiXAX/47qrNc1zLG3N6cdKqsfhQAABUegaUMdaofJVN+3cZcP3jyDEOcAQC4QgSWMlarerDUDgsWm40RQwAAXCkCixdcFRduLrceSvfGjwMAoMIhsHhBr8Y1zeXCHWne+HEAAFQ4BBYvuKFNnLlcu/+EHM/K8caPBACgQiGweEFCVDVpGl1ddP64rYczvPEjAQCoUAgsXtIsNsxcjnn7e/khmVoWAABKgsDiJc2iiwKLuv2/67z1YwEAqBAILF7SNKZoMUSVeS5fbDrOGQAAeITA4iUtzw9ttjt06qy3fjQAAJUvsKxYsUKGDBki8fHx4ufnJ3PmzLns/p9//rlcd911Urt2bQkPD5cePXrIggULnPZ56qmnzGsV31q0aCEVSYNaoWaqfrvtRzItPR4AACp0YMnOzpZ27drJ1KlTPQ44Gli+/vpr2bhxo/Tr188Ens2bNzvt16pVK0lJSXFsK1eulIrm2qti5OZOdc31HSkEFgAAPBUoJTRo0CCzeeqVV15xuv3ss8/KF198IV999ZV06NDh5wMJDJTY2Fip6FrGh4tsFNlxhOHNAAD4bA1LYWGhnD59WqKiopzu37Nnj+lmatSokdx2222SlJR0ydfIycmRzMxMp628aBUfYS7pEgIAwIcDy4svvihZWVlyyy23OO7r1q2bzJw5U+bPny/Tpk2TxMRE6dOnjwk2rkyZMkUiIiIcW0JCgpQXV8UVDW9OyTgn4z/cZPXhAABQLng1sHz44Yfy17/+VT755BOJjo523K9dTDfffLO0bdtWBg4caOpd0tPTzX6uTJ48WTIyMhxbcnKylBdhIVUcI4bm/ZgiJ7NzrT4kAAB8ntcCy6xZs2TcuHEmhPTv3/+y+0ZGRkqzZs1k7969Lh8PDg42I46Kb+XJu2O7Oq5vTjpl6bEAAFAeeCWwfPTRRzJmzBhzOXjwYLf7a5fRvn37JC6uaNHAiqZW9WDHaKFNBBYAAEo/sGiY2LJli9mU1pvodXuRrHbXjBw50qkbSG+/9NJLplYlNTXVbNqVY/fAAw/I8uXL5cCBA7J69WoZNmyYBAQEyIgRI6Si6lS/hrmcunSfvL58n2SczbP6kAAAqDiBZcOGDWY4sn1I8qRJk8z1J554wtzWOVSKj/B54403JD8/X8aPH29aTOzbfffd59jn0KFDJpw0b97cFOPWrFlT1q5dayabq6iuaxnjuD7lm51mfSGm6wcAwDU/WwX4ltRhzTpaSFttylM9y/gPNsm8rSmO2+sfvVaiw0MsPSYAAHzx+5u1hCz015taydNDW0vs+ZCyK831MG4AACo7AovFxbe/715fOtSLNLd3pRJYAABwhcDiA5rFFE0mt5PAAgBA6awlhNLXpk7RdP0LtqdKaFCA1KsZKmN7N+RUAwBwHoHFB/RrES0tYsNMC8s7aw6a+/pfFS31a4ZafWgAAPgEuoR8QIC/nzz/27ZSPfjn/Pj5psOWHhMAAL6EwOIj2taNlHl/6i3DOtQxtxftSLP6kAAA8BkEFh+iXUAPDmxuru9IyZSF21OZTA4AAAKL74mLCJGIqlXM9Tvf2yh/m7tD8goKrT4sAAAsRQuLj/Hz85PaYcGO22+vOiA3T18jufmEFgBA5UVg8UEPX99CosOCpVb1IHN7S3K6vL0q0erDAgDAMgQWH10Ycf1f+suGx64zU/er4msOAQBQ2RBYfNzAVrHi5yfy46EMueX1NfIto4cAAJUQgcXHaT1Lt4ZR5vr6xJMy8eMtknkuz+rDAgDAqwgs5cBLt7R3TN+flZMvH69PtvqQAADwKgJLOVAnsqp8OaGXo55lLvUsAIBKhsBSTuhw5wGtYkw9yw/J6XI4/azVhwQAgNcQWMqR6LAQ6dKgqJ7lj+9tkMTj2VYfEgAAXkFgKWeeHdbazIS77XCm3P7fdXIur8DqQwIAoMwRWMqZJtFh8tWE3mZiuUOnzsoMJpQDAFQCBJZyqF7NavLIoBbm+vPzd0nPKYvl1W/3WH1YAACUGQJLOXVju3hpWCvUXD+ScU7++e1uM+QZAICKiMBSTgUG+Mvrt3eSPk1rOe678V8r5fE526Sg0GbpsQEAUNoILOVYs5gweW9sN7m7b2Nze//xbHlv7UFZs++E1YcGAECpIrBUAON6N5SaoUUrO6vf/3ed9Hl+iWw/kmHpcQEAUFoILBVAzerB8u7YrtKveW3Hfcknz8rf5+4Qm43uIQBA+UdgqSBaxUfI22O6ylsjO8vvutUz963df1Lufn+TJJ88Y/XhAQDwixBYKpj+LWPk2WFt5LHBV5nb87enytCpq2T/sSyrDw0AgCtGYKmgxvZuKK+N6CCNaofKiexcueX1tbJ2P8W4AIDyicBSgRdL1LladBRRaFCAHM/Kkd+/tU4+/j5JNiWdsvrwAAAoEQJLBVcnsqrMGd9LujaIkvxCmzz82Vb59X9Wy6IdaXL09DnZlXra6kMEAMAtP1sFGEaSmZkpERERkpGRIeHh4VYfjk/68VC63PjvVRfdH+jvJ/Mn9jFrFAEA4Kvf37SwVBItYl1/ELTVZfbmw/L9gZOSm1/o9eMCAMATBJZKIijw57e6c/0aclXczwFm6tJ9cvP0NTLo1RVyKjvXoiMEAODSCCyVyNTfdTRB5fnftpV3xnSR+65tarqE7PYdy5YOf18kLy/cJe+uOSCFrEkEAPAR1LBUctsOZ8ib3+2XqlUCZNb3yRc9/rebWsnIHg0sOTYAQMWWWYIalkCvHRV8Uus6EfLq8A6mNSUsJFDe/C7R6fEnvtgut3evb4ZJAwBQbrqEVqxYIUOGDJH4+HjzJTZnzhy3z1m2bJl07NhRgoODpUmTJjJz5syL9pk6dao0aNBAQkJCpFu3brJ+/fqSHhp+AX9/P/nL4JbylxuKZsgtruuzi+WP721gXSIAQPkJLNnZ2dKuXTsTMDyRmJgogwcPln79+smWLVtk4sSJMm7cOFmwYIFjn48//lgmTZokTz75pGzatMm8/sCBA+Xo0aMlPTz8QuP6NJStTw2Q5Q/2leYxRUOdj53OkQXb06TXc0vkpqmr5N6PNsuhU6xPBAAoJzUs2sIye/ZsGTp06CX3efjhh2XevHmybds2x33Dhw+X9PR0mT9/vrmtLSpdunSRf//73+Z2YWGhJCQkyL333iuPPPKI2+NgHpaysXB7qtz53kaXj1ULCpDujWpKVGiQPDOstQQHBpTRUQAAKiqfmodlzZo10r9/f6f7tPVE71e5ubmyceNGp338/f3Nbfs+F8rJyTG/ZPENpW9Aq1h56PrmLh87k1sgS3Yelf9tPCT3frhZFmxPleycfN4GAECZKPOi29TUVImJiXG6T29ryDh79qycOnVKCgoKXO6zc+dOl685ZcoU+etf/1qmx40i9/RtIr/rWk8mffKD1AwNknpR1aRv82j599I9su1wphxOPysLd6SZrVlMdflyQm8JDvSnSBcAUKrK5SihyZMnm5oXOw0/2oWEshFZLUhmjO7idN/rt3c2l1c/v1SSThbVs+xOy5Lb3lonO45kSruECPnjNY2lX/No3hYAgO8HltjYWElLS3O6T29rX1XVqlUlICDAbK720ee6oqONdIP1dBK6Rz/fKlk5+XL0dI5sPFi0EvTa/SdlZ+ppaVc3Uk5m58pHd3aX6sHlMh8DAHxAmdew9OjRQxYvXux036JFi8z9KigoSDp16uS0jxbd6m37PvBdWni75IG+MuXXbS56LP1MnizffUy2Hs6Q/56f30VrvDcePCk5+QUWHC0AoNIElqysLDM8WTf7sGW9npSU5OiuGTlypGP/u+66S/bv3y8PPfSQqUn5z3/+I5988on8+c9/duyj3TtvvvmmvPPOO/LTTz/J3XffbYZPjxkzpnR+S5S5a5rVlqCAoo9Th3qRMrhtnNPjry3ZY0YdzVx9QH4zbY388b2NLud1OZGVI3uPnpZzeQWyZt8JlgcAAFzZsGadBE7nVLnQqFGjzIRwo0ePlgMHDpj9ij9HA8qOHTukbt268vjjj5v9itMhzS+88IIp0m3fvr289tprZrizJxjW7Bs0aLy4YLfc06+xnM0tkAkfbZYRXRIk+dRZsyL0hfo1ry1t6kSYrqTfd68vn286LDNWJUqAv5+ZA2ZHSqY8MKCZTPhVU0t+HwBA2SrJ9zdrCaHMaWtJvxeXSUrGuSt6/vtju5kRSCv3HjdrHg1q49x6AwAonwgs8Dlr95+Q99YclISoajKia4IMfm2lKdRtULOaHDhRsllz3x7TRX5KyZSMM3kyrk8jqR1GATYAlEcEFvi8zUmn5Ej6ObmhTaxk5xbVq8RFhMiJ7FxpWDNUluxMM8sBrNl/4rKv0zIuXL66t7e5PvfHI/L+2oMSUiVAxvRqIIt2HJXHBl8loYxOAgCfRGBBhaDh47E5RUs69L8qWuIiqsp7aw9etF/HepGyKSnd5Wvcd21T+b+2cWa0ktbJaJgBAJS/wMLEGPBZreJ//vA+O6yNRIeHmDqYb38qmrPn7r6NZdqyfZcMK2rV3uOm5WXfsWw5dOqsPHrDVZJ4PFuax4aZEUi6SjUAwPcRWOCz2taNlKHt4yUqNNiEFaVrG0VWqyJ39GkkTaKry/YjmbJi9zHR3PHn/s1kbJ+GMuLNdfJDclGI2XB+IjulQ6p1U9r9pGsf3d23idzaJUH+uWi3WYH6iSGtpGGtUMdzcvMLZevhdOlYr8ZFyw1knsuTo5nnpEl00arWAICywyghlGvaSnI6J18iqlZx3KdDqrUL6PNNh8waR+7oyKOzeUUT2bWIDZNnhrUxRcJpmefk3TUHHTP66mPf7TluupZCgwLkxn+vkp2pmfLq8A7yqxbRcuBEtlkUUltwBraMlRPZOTLlm50y6bpmclVcuBw8kW2eP6JrPTN0GwAqu0yGNQNF9h3LEo0G2mry0fpkmfV9klkb6Wxuvnx/4OfWl1+qUe1QST55RvIKiqY10mHY2TkFZnHIakEBsuGx/tLyiQXmsem/7yTXt3a97AQAVCaZBBbAs9YZHYWkXUFXN6ste49mycgZ66VkUymWnBYQa7Dp1aSm9GlaW2pVDzZdWufyC82K2KkZ56RBsW4pVVBoMy0+8ZFVTTD6YF2SdGsUxeKSAMo1AgtwhbSLR5cY0GCg88Roncs1LxTN2qxDpTU42LuJtOVEu4BcCQsONF1VnmhUK1Q0I2lXUpUAPxNmftOxrlzXMka6NYySGqFB8tLCXfKvJXvllVvbm+4u+8zBDw5sLrtST5tuJw05WnNz21trJdDfX975Q1cJCvSX/IJC+SnltClWzi8slAcGNL+oHudCGorqRFYtcVGytmjp+dP5dgDAHQILUIr+s2yvbDhwSl4Z3l7CQ4pqZfaknTah5sH//SCZZ/OlY/0a0rNxTXljxX4Z2CpG5v6YYupV7EKqFIWg/ceyS/zzR/ao7whJql5UNUk6efFkexpy1uw7LkfOzyisrUa6tMGTX26XzcVGUi3889XSLCbMFA1r/c73B05K1rl8Ca9axRQXf/nDEXng0x9kQMsYmXpbR6lyfo0odw4cz5YBr6ww52j1I78yYQkALofAAlhsd9ppuenfq0ytyj9vbW/u23Y4Q/7vXyvN9etbxcr87alOz2mXECnPDG1tWinum1W0uGhZqFujqqnp0UDVuHaoGfJtpyHlh0PpkpaZY263rhMut3apJ6v3HpdbOieYRS4XbE+V99cdlNZ1ImTyoKvMfqfP5clTX+6QzzYdMrc/+WMP6dowyqn77cP1SablZnSvBmZOHW0Z2nDwpAzv4r4IOf1MrplDh3l0gIqFwAL4gNPnWzACz7dQaItG26cWmutvjuwsgQF+5gv8yy1HzPBrXTOpd9NajnCjrRw7U087Xi/Q30/yC38usFn2QF95fcV+0+KiI5C0y+d4Vq4JPkPaxsmr3+4x3VI6uqn463hKW4XO5RU63efqGBJPZMuYt7932q96cKD8rls9ual9vLSKj5D521Llrvc3msfCQwJN15rOcKye+3Ubs8/CHammi00LlnW9qLz8QqkaFCAZZ/Pk2peWm9/zscEtxSY2U/ujNNzd++Fm05XWIjZc/j609UUtO9oapkPdb+9eX+7rz0KagC8hsAA+6qH//SDHTufI67d3dnyxauuDrlgdG1E014zdiawcWbXvhGn10G6btnUiZfbmQ/LUVzvk1s4J8o/ftnXaXxdeL16bol002lpyQ5s4afqXbxz36xe/zj2jazulZl68IKWGkpljukrTmOoydOqqixatDAsJNDU69q4nd7o2iJL1B05e8nGdT0eHouuIqgt1aVBDbmwXL49/sd3pfq3d2XTwlCzeedTp/um/7yjXt45zLLqpXXA3vPad4/HP7+kpAX5+JtR9siFZXliwy3SlTezfVP67MlE6168hbepGyCOfbTUtRDqE3U7Dkb53nerXcNlNpi1Gn25Ilgm/amJGogFwj8ACVGDaYlC3RjXT+uCpzzYeknlbU+TJIS1NMa22+thbPR4Z1ELa1Y00XUWFNptp/WhUu7rjS/rvc3dISGCAowtr1p3dTRh65POtTj9jfL/Gctc1jWXmqgMmfOjq2jq7cHHaymFfXuFPv2oi01fsN4XCpenmTnVlWMc6Mvrt712+tvY+dWkQJesSTzoCmnZ1afjRUVo6g/LT834yjy2YeLXUr1nNhJkXF+4yI8j6XxUjb43q7PSaO45kOoLR4DZx5vV6Na0lMWHBpp5Jw4/WMGmo1O62HSkZUrVKoPRoXNPxnmpgDTtfI6UTEprRY16er0ePT0fLNa5dnVmg4RUEFgAe0VYIT+pCtIbk+le+k8bRoabrSr+4525NMV1e32xNkXo1q8nE/s2cnqOjnu79aJOEBgWamh4t6v120jXyv41FdS46gd4XWw476nXu6dtYhnaoYwqCNQBM+GiTU5HyF+N7mRmKtWVk1vfJjjDkbkRWjWpV5NSZvFL9RGioqeLvZ1Ya11YXd4t06qrk2p2lIc4+27KKDQ9xtHL1blJL3h/XTaYv3yfPfbNTEqKqysd39jD1PVo0rYFm8U9ppsUp/Wye3PXeRtMypDVGq/Ydl44JNSSiWhXTWqXdeXpOzND9fSfMxIo9mxR1N17O1KV7TavT00NbO7UuXfiZ0ZFglwpT+vgjn/0oLePDZXDbeImPCLloVNr9n/xg6pc+vauHRIc5tyyicslkHhYAZfF/3xpUruT/+vVLTL+zggMDXA4lr1Yl0ISe4o6kn5V/zN9pvrC1BWhUzwZOjxdfC0pbUu75YJNsP5Lh1IVlL/4d/sYaWbv/pKO+RruaNBzN+zHFzIasL9O3ebQsuaCL6cKWG62nudTsyfrF7Gk32eUmINSgZ58LSGuAtDj6ZHauYx+dZblacKAJSqpW9SBTuxQTHmyKuXWOHu2yss/eXLwbbX3iSTOabVDrODNsX4uktWZIa4h+PJQhj84uajXT7spFf77aLECqLWs1qgWZ1iDtnhw943u5pnlteerGVrLhwElTO6TvUXR4sAky05bvk+fn73L8XD3f+rMevr6FCXopGWelx5Ql5rGxvRvK4//X0u15yTiTJ2mnz5kwq/Vdq/cdlz/0amhaCu2fSz1llyre1hA38ePN5vn3D2h+yZ9zJjdfnpn3kwmF3RoVtX6hbBFYAFRa2v2lQUdrcLQ1SP/v/oUFO2Xq0n2mO2zen3o7akx0TpvJn/0oE69rZkZBLdt1VDYePCU9G9cyEwp+uvGQacWICg2SptFFX5ba8qNfzLXDgs3EfTofT68mtUw3mn55rtp7wvz8rYczXB6f1s/Y17pSbesWhYWKQCdDPHzqrGl5cmVUj/ryTrEh+lqA/dIt7WXW+iRT56T1Tn2a1pLG0dUlNDjQnOdTZ3Ll3o82m7A4647uMurt9aYYfHiXBNM9989vd5s1xZR2eY7p1fCin/vO6gNmeL9qUydC/nZTK+lQr4bjcZ2r6PPNh+Uf3+yUE+fDYeKUG8xn56eUTLMN61DH3Nb3WINtTHiIOTZtGXM3r1Fx+nydZVuPo3i3rr0G7VR2rvldL5w80hMa3MvbdAIEFgC44P+ctWtEWwmqBQV65efp3D36Zfbyot3y76V7TYvAqJ71pX7NUPnX4j3y0qLdji9GbS15Z80B80Ws3Ufa2tPn+aXmce3eWfZAP4kOC5Y9R7Nk4CsrzP0t48Llrr6NzReczrkzcdZm88Xdr0W0ZJ7Nc9To6H47Uoq+0JUuHpruYReZBorMc/km6LkqivZFOiouONDftPzc06+xqRm6490NJgTY6Zf6879pKx+uSzKj9XTfGasSnV7n1eHtTRfZ7rQsc3vG6M6m+1RbYOwBSf3xmkZSPyrUtES98Nu2cvpcvqzcc8x0qWkLkLbu6JD+jDO5suVQhlmsVY3omiBTft1W3l1zwNRMadjQFiw9z3vSsuSre3ub1i/lKhCdzS0wLUr2gKJBXeeFGtAyVl6+td1Fn3OdKVtbOvXzpzSQ/2X2NlN3Zq+lsgKBBQB8RF5Boenm0e6I4l82z3y9Qwa2inUM0b6QFvrq/DfadTWkXbzj/g/WHZSkE2dkXJ9GppWneBdZgc3mGMGkt7U+RoOODnnXmp/fdqprHntt8V5TTDyoTaz845tdkpNfYFqQdMTW419sM11Qf7q2qZlB2e7NFftN/ZEOz9duN60duqlDvLy/NqloNFf7OvLUl9tN0XZxWsxdvPh6UOtY82WuP/MPMzc47avBSMOeveaoJDNGu6NFzFc3rSXfbEu9qLvME4PbxplC9Qt/v8vpUC9S9qZlXfJ30FYb+6zVrmhY1W7UCf2amPmL7n5/o6mD0m42fY8a1AyVf/+uo+xKy5QpX+80ow3tr6vnuGO9SBN29Fz3fWGZed+0a3B8vyYy7p3v5dufjjqWC9HX1G7X9gmR5j4NeN4Y7UZgAQBckS3J6aY2ZXTPBo45hC4cbv/Gd/vltq71TWD6+Psk+b928SYQaH2KfilqQPnTR5vNXDzPDmtjuly060a7i579dRvH//1r68Lry/fLy7e0c9SMbEo6JcNfX2vC1Es3t5Olu46ZbrNnv/5JvthyxPEFqyFOv7R1Zuapv+touu20WLl4UbOddr/o6DbtZtKJD//4XtGcQJ7QFi9XXVza+qEzQ2vLirsWK20FMUtw2ER2pZV8TqQrVT040LSoFQ+M2s2mq89P/Nj15JS/7ljHPE9n19bwou+rzinl7+cnL97SzoTK0kRgAQBYRusxtBtKh0dfyezEWqSrI9BcFdHql6d9+PeFtFVJR5Fp/VLN0GA5knHWdNFoobH9ORqeBvxzhew/nm2CkI6g0sJmHYFmL9iee29vMweRBpw1k39l5uXRYGR359WNZPKgFqb14nhWjqlp+fj7ZFP/pIEgt6DQUVSsLTNaNK6/i04UOejV78zvZ/fY4KtMa9mSnWmmxUn30/mANBgMaBUjM1YecDmP0f3XNXN0K9rpWmPataTHqiPHSrqQq57uyzUg6RxOH/+xu5mpurQQWAAAuAStGZq2bJ8M7RBvuuo+33TYFPtq8bO2SGgRtRbaai2MFlNrENJ5jPS+P/RuaFod3I1qyissdLmf1hwFV/GXWeuTTdjRbjd7S5bOb6TD/7W1yG5z0ikZ9p/V5rp272kY0/mMdILElXuOm9F0GsxCgwJlXJ+GjnoXbalatvOodKhfw+yntTN9m9eWN79zrtXRGift5vrkrh5mOoH7Zm12dC3p0hx9m0WbLrSP1ifJVXHhMnNMl0sGxitBYAEAoILQYtngQP8SjUa6VMuXBhldTX3V3uPSqFZ1M2qtOK132XY4U1rFhzu1jqVmnJPQ4IBSDSuKwAIAAHxeSQJL+RqwDQAAKiUCCwAA8HkEFgAA4PMILAAAwOcRWAAAgM8jsAAAAJ9HYAEAAD6PwAIAAHwegQUAAPg8AgsAAPB5BBYAAODzCCwAAMDnEVgAAIDPC5QKQJfMtq/6CAAAygf797b9e7zCB5bTp0+by4SEBKsPBQAAXMH3eERExGX38bN5Emt8XGFhoRw5ckTCwsLEz8+v1NOfBqHk5GQJDw8v1deuiDhfnCs+W9bj3yHnq7x8tjSCaFiJj48Xf3//it/Cor9k3bp1y/Rn6BtDYOF88dmyHv8WOVd8tirWv0N3LSt2FN0CAACfR2ABAAA+j8DiRnBwsDz55JPmEu5xvjzHuSoZzhfnqqzw2Sof56pCFN0CAICKjRYWAADg8wgsAADA5xFYAACAzyOwAAAAn0dgcWPq1KnSoEEDCQkJkW7dusn69eulslmxYoUMGTLEzESoMwnPmTPH6XGt237iiSckLi5OqlatKv3795c9e/Y47XPy5Em57bbbzERDkZGRMnbsWMnKypKKZsqUKdKlSxcz63J0dLQMHTpUdu3a5bTPuXPnZPz48VKzZk2pXr26/OY3v5G0tDSnfZKSkmTw4MFSrVo18zoPPvig5OfnS0Uzbdo0adu2rWMSqh49esg333zjeJxzdWnPPfec+fc4ceJEzpcLTz31lDk/xbcWLVpwri7h8OHD8vvf/978XdK/423atJENGzb41t95HSUE12bNmmULCgqyzZgxw7Z9+3bbHXfcYYuMjLSlpaVVqlP29ddf2/7yl7/YPv/8cx1RZps9e7bT488995wtIiLCNmfOHNsPP/xgu/HGG20NGza0nT171rHP9ddfb2vXrp1t7dq1tu+++87WpEkT24gRI2wVzcCBA21vv/22bdu2bbYtW7bYbrjhBlu9evVsWVlZjn3uuusuW0JCgm3x4sW2DRs22Lp3727r2bOn4/H8/Hxb69atbf3797dt3rzZnP9atWrZJk+ebKtovvzyS9u8efNsu3fvtu3atcv26KOP2qpUqWLOn+JcubZ+/XpbgwYNbG3btrXdd999jvs5Xz978sknba1atbKlpKQ4tmPHjnGuXDh58qStfv36ttGjR9vWrVtn279/v23BggW2vXv3+tTfeQLLZXTt2tU2fvx4x+2CggJbfHy8bcqUKbbK6sLAUlhYaIuNjbW98MILjvvS09NtwcHBto8++sjc3rFjh3ne999/79jnm2++sfn5+dkOHz5sq8iOHj1qfvfly5c7zo1+IX/66aeOfX766Sezz5o1a8xtDSj+/v621NRUxz7Tpk2zhYeH23JycmwVXY0aNWxvvfUW5+oSTp8+bWvatKlt0aJFtmuuucYRWPhsXRxY9MvTFc6Vs4cfftjWu3dv26X4yt95uoQuITc3VzZu3GiavYqvWaS316xZU3pNXOVcYmKipKamOp0nXRdCu8/s50kvtXmwc+fOjn10fz2f69atk4osIyPDXEZFRZlL/Uzl5eU5nS9tpq5Xr57T+dLm2JiYGMc+AwcONIuObd++XSqqgoICmTVrlmRnZ5uuIc6Va9qdqN2FxT9DivN1Me2y0K7sRo0ama4K7WrlXF3syy+/NH+fb775ZtMF3aFDB3nzzTd97u88geUSjh8/bv6AFv/SUHpb3zgUsZ+Ly50nvdR/BMUFBgaaL/GKfC51FXGtL+jVq5e0bt3a3Ke/b1BQkPmHfbnz5ep82h+raLZu3WpqeXTmzLvuuktmz54tLVu25Fy5oIFu06ZNplbqQny2nOmX6cyZM2X+/PmmVkq/dPv06WNWBuZcOdu/f785R02bNpUFCxbI3XffLX/605/knXfe8am/8xVitWbAV/9PeNu2bbJy5UqrD8WnNW/eXLZs2WJao/73v//JqFGjZPny5VYfls9JTk6W++67TxYtWmQGAeDyBg0a5Liuhd0aYOrXry+ffPKJKRqF8/9cacvIs88+a25rC4v+7Zo+fbr59+graGG5hFq1aklAQMBFozf0dmxsrDfem3LBfi4ud5708ujRo06P64gXrSivqOdywoQJMnfuXFm6dKnUrVvXcb/+vtrdmJ6eftnz5ep82h+raLTFqUmTJtKpUyfTctCuXTt59dVXOVcX0C4f/XfUsWNH83+uummwe+2118x1/b9dPluXpq2azZo1k7179/LZuoCO/NFWzeKuuuoqRxear/ydJ7Bc5o+o/gFdvHixUwrV29q/jiINGzY0H8bi50lrLbTP0n6e9FK/oPUPrt2SJUvM+dT/66lItC5Zw4p2a+jvqOenOP1MValSxel86bBn/cNQ/HxpN0nxf/z6f9U6VPDCPyoVkX4ucnJyOFcXuPbaa83nQluj7Jv+X7HWZtiv89m6NB1eu2/fPvPlzL9DZ9ptfeH0C7t37zYtUj71d75USncr8LBmrYKeOXOmqYC+8847zbDm4qM3KgMdlaDDa3XTj8zLL79srh88eNAx3E3PyxdffGH78ccfbTfddJPL4W4dOnQwQ+ZWrlxpRjlUxGHNd999txn6t2zZMqfhlGfOnHEaeqpDnZcsWWKGNffo0cNsFw5rHjBggBkaPX/+fFvt2rUr5LDmRx55xIygSkxMNJ8dva2jChYuXGge51xdXvFRQpwvZ/fff7/5d6ifrVWrVplpAnR6AB25x7m6eJh8YGCg7ZlnnrHt2bPH9sEHH9iqVatme//99x37+MLfeQKLG//617/Ml4vOx6LDnHV8eWWzdOlSE1Qu3EaNGuUY8vb444/bYmJiTMC79tprzZwaxZ04ccJ8cKtXr26G544ZM8YEoYrG1XnSTedmsdN/4Pfcc48Zvqt/FIYNG2ZCTXEHDhywDRo0yFa1alXzR1b/+Obl5dkqmj/84Q9m/gf996WhTD879rCiOFclCyycr5/deuuttri4OPPZqlOnjrldfF4RzpWzr776yvyPkv4Nb9Gihe2NN95wetwX/s776X9Kp60GAACgbFDDAgAAfB6BBQAA+DwCCwAA8HkEFgAA4PMILAAAwOcRWAAAgM8jsAAAAJ9HYAEAAD6PwAIAAHwegQUAAPg8AgsAAPB5BBYAACC+7v8BpVwyORPpXHEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(torch.tensor(losses[:-525]).view(-1, 1000).mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "929738a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6091"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "153c8b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  0.8648499250411987\n",
      "val loss:  1.742585301399231\n"
     ]
    }
   ],
   "source": [
    "train_loss = torch.tensor(losses[-1000:]).mean().item()\n",
    "val_loss = torch.tensor(val_losses).mean().item()\n",
    "print(\"train loss: \", train_loss)\n",
    "print(\"val loss: \", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "a023a4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3040407\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for param in model.parameters():\n",
    "    count += param.numel()\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "fe296544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONTEXT_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "c4b6f904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07/12/2025, 11:06:52] Yi Hein: how will we ever know when FEP assumptions hold true? if not we will just be applying it wrongly of elex for mutation forgies to trying one stemally big pretty solve sure aps. i sears the unforts disgruent in saying bas justify than relativing withs a is thats not foruthm sritting in wed between!\n",
      "\n",
      "[13/01/2026, 20:24:11] Haziq KXHL: the one automatism because say really what then our can squine to guys goodor. response)\n",
      "\n",
      "\n",
      "\n",
      "this see you so completely mathing my things you at restro usclubinated anno, a second place, then interested, and view call in this over that field is\n",
      "\n",
      "[09/01/2026, 01:30:30] Haziq KXHL: *30 being about!\n",
      "\n",
      "[10/12/2025, 07:33:19] Haziq KXHL: i an and job this'olrakis\n",
      "\n",
      "[10/12/2025, 20:06:43] ~Adam: I to even needers?\n",
      "\n",
      "[15/08/2025, 20:51:27] ~Adam : you canLt the gead of gods\n",
      "\n",
      "[09/12/2025, 04:57:31] ~MS: I think? humanagring\n",
      "\n",
      "[09/01/2026, 04:02:30] ~manav: Ahaught\n",
      "\n",
      " 3nier and a more a quite with the the cave and set of but is great and at a do i selligents a largent for more of <This message was edited>\n",
      "\n",
      "[10/12/2025, 08:21:50] Niv Ucl Builder: i make of you \n"
     ]
    }
   ],
   "source": [
    "generated = \"[07/12/2025, 11:06:52] Yi Hein: how will we ever know when FEP assumptions hold true? if not we will just be applying it wrongly\"\n",
    "# context = [tokeniser['<s>']] * CONTEXT_LENGTH\n",
    "context = [tokeniser[c] for c in \"[07/12/2025, 11:06:52] Yi Hein: how will we ever know when FEP assumptions hold true? if not we will just be applying it wrongly\"]\n",
    "for i in range(1000):\n",
    "    pred = model.generate(torch.tensor(context).unsqueeze(0).to(device))\n",
    "    token = pred.item()\n",
    "\n",
    "    context = context[1:] + [token]\n",
    "    generated += detokeniser[token]\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
